<DOC>
<DOCNO>EP-0651321</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Superscalar microprocessors
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F9302	G06F930	G06F9302	G06F932	G06F938	G06F932	G06F938	G06F930	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G06F	G06F	G06F	G06F	G06F	G06F	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F9	G06F9	G06F9	G06F9	G06F9	G06F9	G06F9	G06F9	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A superscalar microprocessor is provided which 
includes a integer functional unit and a floating point 

functional unit that share a high performance main data 
processing bus. The integer unit and the floating point 

unit also share a common reorder buffer, register file, 
branch prediction unit and load/store unit which all reside 

on the same main data processing bus. Instruction and data 
caches are coupled to a main memory via an internal address 

data bus which handles communications therebetween. An 
instruction decoder is coupled to the instruction cache and 

is capable of decoding multiple instructions per 
microprocessor cycle. Instructions are dispatched from the 

decoder in speculative order, issued out-of-order and 
completed out-of-order. Instructions are retired from the 

reorder buffer to the register file in-order. The 
functional units of the microprocessor desirably accommodate 

operands exhibiting multiple data widths. High performance 
and efficient use of the microprocessor die size are 

achieved by the sharing architecture of the disclosed 
superscalar microprocessor. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
ADVANCED MICRO DEVICES INC
</APPLICANT-NAME>
<APPLICANT-NAME>
ADVANCED MICRO DEVICES INC.
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
JOHNSON WILLIAM M
</INVENTOR-NAME>
<INVENTOR-NAME>
WITT DAVID B
</INVENTOR-NAME>
<INVENTOR-NAME>
JOHNSON, WILLIAM M.
</INVENTOR-NAME>
<INVENTOR-NAME>
WITT, DAVID B.
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
This invention relates in general to microprocessors and, more
particularly, to high performance superscalar microprocessors.Like many other modern technical disciplines, microprocessor
design is a technology in which engineers and scientists continually
strive for increased speed, efficiency and performance. Generally
speaking, microprocessors can be divided into two classes, namely
scalar and vector processors. The most elementary scalar processor
processes a maximum of one instruction per machine cycle. So called
"superscalar" processors can process more than one instruction
per machine cycle. In contrast with 
the scalar processor, a vector processor can process a
relatively large array of values during each machine cycle.Vector processors rely on data parallelism to achieve
processing efficiencies whereas superscalar processors rely
on instruction parallelism to achieve increased operational
efficiency. Instruction parallelism may be thought of as
the inherent property of a sequence of instructions which
enable such instructions to be processed in parallel. In
contrast, data parallelism may be viewed as the inherent
property of a stream of data which enables the elements
thereof to be processed in parallel. Instruction
parallelism is related to the number of dependencies which a
particular sequence of instructions exhibits. Dependency is
defined as the extent to which a particular instruction
depends on the result of another instruction. In a scalar
processor, when an instruction exhibits a dependency on
another instruction, the dependency generally must be
resolved before the instruction can be passed to a
functional unit for execution. For this reason,
conventional scalar processors experience undesirable time
delays while the processor waits pending resolution of such
dependencies.Several approaches have been employed over the years to
speed up the execution of instructions by processors and
microprocessors. One approach which is still widely used in
microprocessors today is pipelining. In pipelining, an
assembly line approach is taken in which the three
microprocessor operations of 1) fetching the instruction,
2) decoding the instruction and gathering the operands, and
3) executing the instruction and writeback of the result,
are overlapped to speed up processing. In other words,
instruction 1 is fetched and instruction 1 is decoded in 
respective machine cycles. While instruction 1 is being
decoded and its operands are gathered, instruction 2 is
fetched. While instruction 1 is being executed
</DESCRIPTION>
<CLAIMS>
A superscalar microprocessor (200) comprising:

a multiple instruction decoder (210) for decoding multiple instructions in the
same microprocessor cycle, said decoder decoding both integer and floating point

instructions in the same microprocessor cycle; and
a common data processing bus (535) coupled to said decoder (210);
characterised by
:

an integer functional unit (220) coupled to said common data processing bus
(535);
a floating point functional unit (230) coupled to said common data processing
bus (535);
a common reorder buffer (240), coupled to said common data processing bus
(535), for use by both said integer functional unit (220) and said floating point

functional unit (230); and
a common register file (235) including at least one register for use by both said
integer functional unit (220) and said floating point functional unit (230), and coupled

to said reorder buffer (240) for accepting instruction results which are retired from said
reorder buffer (240).
A microprocessor as claimed in claim 1, wherein said integer functional unit
(220) includes at least one reservation station (540, 545).
A microprocessor as claimed in claim 1 or claim 2, wherein said integer
functional unit (220) includes two reservation stations (540, 545). 
A microprocessor as claimed in claim 1, 2 or 3, wherein said floating point
functional unit includes at least one reservation station (560, 570, 580, 590).
A microprocessor as claimed in any preceding claim, wherein said floating point
functional unit (230) includes two reservation stations.
A microprocessor as claimed in any preceding claim, further comprising:

   a branch prediction functional unit (825) coupled to said data processing bus
and shared by said integer functional unit (220) and said floating point functional unit

(230).
A microprocessor as claimed in any preceding claim, wherein said floating point
functional unit (230) processes operands exhibiting multiple sizes.
A superscalar microprocessor (200) as claimed in claim 1, wherein:

said integer functional unit (220) includes a plurality of reservation stations
(540, 545) for enabling out-of-order instruction execution by said microprocessor;
said floating point functional unit (230) includes a plurality of reservation
stations (560, 570, 580, 590) for enabling out-of-order instruction execution by said

microprocessor; and
said common reorder buffer (240) is configured to enable instructions to be
processed speculatively and out-of-order;
said microprocessor further comprising:

a branch prediction unit (825) coupled to said data processing bus, for use by
both said integer functional unit (220) and said floating point functional unit (230) to 

speculatively predict which branches in a computer program are taken; and
a load/store functional unit (530) coupled to said data processing bus, for use by
both said integer functional unit (220) and said floating point functional unit (230) to

permit loading and storage of information.
A microprocessor as claimed in any preceding claim, wherein said data
processing bus includes:


a plurality of opcode (OPCODE) buses;
a plurality of operand (A,B OPER) buses;
a plurality of instruction type buses (TYPE);
a plurality of result (RESULT) buses; and
a plurality of result tag buses.
A microprocessor as claimed in claim 9, wherein said operand buses include
operand tag 
buses (A,B TAG).
A microprocessor as claimed in claim 9, wherein said plurality of operand buses
are buses on which both operands and operand tags are transmitted.
A microprocessor as claimed in any preceding claim, wherein said data
processing bus has a predetermined data width and wherein said reorder buffer (240)

includes memory means for storing entries exhibiting a width equal to the data
processing bus width and entries exhibiting a width equal to a multiple of the data width

of said data processing bus. 
A microprocessor as claimed in any preceding claim, wherein said decoder (210)
further includes dispatching means (820) for dispatching both integer and floating point

instructions in program order.
A microprocessor as claimed in any preceding claim, wherein said floating point
functional unit (230) comprises a single precision/double precision floating point

functional unit.
A microprocessor as claimed in any preceding claim, further comprising:

a bus interface unit (260) for interfacing said microprocessor to an external
memory in which instructions and data are stored;
an internal address data communications bus (250) coupled to said bus interface
unit;
a load/store functional unit (530) coupled to said data processing bus to receive
load and store instructions therefrom, and further being coupled to said internal address

data communications bus (250) to provide said load/store functional unit access to said
external memory;
an instruction cache (205) coupled to said internal address data communications
bus (250) and said decoder (210) to provide said decoder (210) with a source of

instructions; and
a data cache (245) coupled to said internal address data communications bus
and said load/store functional unit (530);
said internal address data communications bus (250) communicating address and
data information among said external memory, said instruction cache (205) and said

data cache (245).
A microprocessor as claimed in any preceding claim, wherein said multiple
instruction decoder (210) is capable of decoding four instructions per microprocessor

cycle.
A microprocessor as claimed in any preceding claim, combined with an external
memory for providing instructions and data to said microprocessor (200).
A microprocessor as claimed in any preceding claim, wherein the common
reorder buffer (240) comprises a content-addressable memory.
</CLAIMS>
</TEXT>
</DOC>
