<DOC>
<DOCNO>EP-0634031</DOCNO> 
<TEXT>
<INVENTION-TITLE>
APPARATUS AND METHOD FOR EYE TRACKING INTERFACE
</INVENTION-TITLE>
<CLASSIFICATIONS>A61B3113	G06F301	A61B50496	G06F300	A61B3113	G06F300	G06F301	A61B50496	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>A61B	G06F	A61B	G06F	A61B	G06F	G06F	A61B	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>A61B3	G06F3	A61B5	G06F3	A61B3	G06F3	G06F3	A61B5	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
An eye tracking interface system for generating communication and control functions as a result of predefined eye gestures is disclosed. The system includes a detecting device adapted to detect bio-electromagnetic signals generated by eye movements. A first processor receives the detected bio-electromagnetic signals, and generate tokens corresponding to said pre-defined eye gestures. A second processor receives the tokens, and generates command signals based on a protocol correlating tokens to desired command signals. Thereafter, a user interface responds to said command signals, and provides control functions in response to said command signals.
</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
UNIV NEW YORK
</APPLICANT-NAME>
<APPLICANT-NAME>
THE RESEARCH FOUNDATION OF STATE UNIVERSITY OF NEW YORK
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
BANDOPADHAY AMIT
</INVENTOR-NAME>
<INVENTOR-NAME>
KAUFMAN ARIE A
</INVENTOR-NAME>
<INVENTOR-NAME>
PILIGIAN GEORGE J
</INVENTOR-NAME>
<INVENTOR-NAME>
BANDOPADHAY, AMIT
</INVENTOR-NAME>
<INVENTOR-NAME>
KAUFMAN, ARIE, A.
</INVENTOR-NAME>
<INVENTOR-NAME>
PILIGIAN, GEORGE, J.
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
This invention relates to systems for utilizing
eye movements for communication and control. More
particularly, the invention relates to systems for
detecting eye movements, deriving signals from such
movements, processing those signals to generate signals
suitable for communication and control, and interfacing
those signals with equipment designed to perform various
functions. The latter may include tele-robotics,
navigation of vehicles including motorized wheelchairs,
those involving "virtual reality", and those related to 3D
computer graphics (e.g., visualization and exploration of
imaginary or unattainable spaces).It is known that eye movements are accompanied
by the appearance of electric signals. The electric
signals generated by movements of the eyes both the
horizontal and vertical directions can be measured by
electrodes. This method is known as electro-oculography
(EOG) or alternatively electro-nystagmography (ENG). It
is used in clinical medicine for studying eye movements
(See, e.g., Bahill, A.T., Bioengineering, Medical and
Clinical Engineering, Prentice-Hall, Inc., Englewood
Cliffs, NJ, 1981; Neuman, M.R., Flemming, D.G., Cheung,
P.W. and Ko, W.H., Physical Sensors for Biomedical
Applications, CRC Press, Inc., Boca Raton, FL., 1977; and
Young, L.R. and Sheena, D., "Survey of Eye Movement
Recording Methods", Behavior Research Methods &
Instrumentation, 7, 5 (1975), 397-429.)EOG is based on electrical measurement of the
potential difference between the cornea and the retina
which is about 1 mv (millivolt) under normal
circumstances. This corneo-retinal potential creates an
electrical field in the front of the head which changes in
orientation as the eyeballs rotate. The resultant
electrical changes can be detected by electrodes placed 
near the eyes. In clinical practice, the detected voltage
changes are amplified and used to drive a plotting device,
whereby a tracing related to eye movement is obtained. As
a measure of eye position, these signals are inaccurate
and noisy and therefore the EOG method has not been used
for implementing eye position tracking.Other methods of eye tracking are known. [See,
e.g., Bahill, A.T., Bioengineering, Medical and Clinical
Engineering, Prentice-Hall, Inc., Englewood Cliffs, NJ,
1981; Neuman, M.R., Flemming, D.G., Cheung, P.W. and Ko,
W.H., Physical Sensors for Biomedical Applications, CRC
Press, Inc., Boca Raton, FL., 1977; Yamada, M., Fukuda,
T. and Hirota, M.r "A New Eye Movement Analyzer: Auto
Calibration and Wireless Transmission", IEEE Medical an
Biolo
</DESCRIPTION>
<CLAIMS>
An eye tracking interface system for controlling task-performing
functions comprising a detection device (50)

adapted to detect bio-electromagnetic signals generated by
eye movements 
characterised in
, a first processor (52)
adapted to receive said detected bio-electromagnetic

signals and, in response thereto, assigning tokens from a
set of symbolic tokens corresponding to eye movement

classifications, thereby producing tokens representative of
said bio-electromagnetic signals, a second processor (54)

adapted to receive said tokens, interpreting said tokens,
and generate command signals based on said interpretation

where said command signals control task-performing
functions.
The system according to claim 1, 
characterised in
 that
said set of symbolic tokens includes symbolic velocity

tokens corresponding to a plurality of eye velocity
classifications.
The system according to claim 2, 
characterised in
 that
said system disengages in response to a symbolic velocity

token corresponding to a saccade velocity classification.
The system according to claim 1, 
characterised in
 that
said system engages in response to symbolic tokens.
The system according to claim 1, 
characterised in
 that
said system reengages in response to symbolic tokens.
The system according to claim 1, 
characterised in
 that
said symbolic tokens are associated with numerical value

tokens.
The system according to claim 6, 
characterised in
 that
said second processor includes a protocol interpreter (54) 

that receives value tokens corresponding to the position of
each eye for determining the distance and depth of an

object.
The system according to claim 7, 
characterised in
 that
said object is displayed on a screen (88) as a two

dimensional representation of a three dimensional space.
The system according to claim 6, 
characterised in
 that
said numerical value tokens indicate displacement, speed or

duration.
The system according to claim 1, 
characterised in
 that
said detecting device detects electro-oculograph (EOG)

signals.
The system according to claim 10, 
characterised in

that during an interaction mode said first processor (52)
receives EOG signals resulting from eye movements and

generates a token from said set of symbolic tokens when
said EOG signals fall within a range of one or more

classification parameters corresponding to a given eye
movement classification that corresponds to said generated

token.
The system according to claim 10, 
characterised in

that said eye movement classifications are established by a
calibration test that determines a set of one or more

parameters corresponding to a given eye movement
classification, where each said eye movement classification

corresponds to a token from said set of symbolic tokens.
The system according to claim 12, 
characterised in

that during an interaction mode said first processor (52)
receives EOG signals resulting from eye movements, and

generates a token from said set of symbolic tokens when 
said EOG signals fall within said classified set of

parameters corresponding to said generated token.
The system according to claim 10, 
characterised in

that said eye movement classifications include a plurality
of velocity classifications that are established by a

calibration test that determines a velocity parameter
corresponding to each said velocity classification, where

each said velocity classification corresponds to a velocity
token.
The system according to claim 1, 
characterised in
 that
said second processor (54) receives serial streams of

tokens derived from plural EOG signal channels and
recognizes combinations of tokens received from said

channels for generating said command signals.
The system according to claim 1, 
characterised in
 that
said second processor comprises a protocol interpreter (54)

which may be initialized for a particular user application
by accessing a particular protocol for interpreting various

tokens and generating command signals.
The system according to claim 1, 
characterised in
 that
said second processor (54) includes a rule-based pattern

learning component for defining distinct token patterns for
generating command signals.
The system according to claim 1, 
characterised in
 that
said second processor (54) includes a rule-based pattern

learning component such that an operator may define

distinct token patterns for generating command signals.
The system according to claim 1, 
characterised in
 that
said system includes a monitor screen (88) displaying a

choice of commands, said commands corresponding to an
operation of a machine, so that an operator may select a 

desired command by providing a predefined operator-personalized
eye gesture.
The system according to claim 19, 
characterised in

that a screen cursor tracks the operator's gaze on said
monitor screen.
The system according to claim 1, 
characterised in
 that
control messages are sent from said second processor (54)

to said first processor (52).
The system according to claim 1, 
characterised in
 that
control messages are sent from a device interface (56) to

said second processor (54).
A method for providing communication or control functions
as a result of eye movements, comprising the step of

detecting bio-electromagnetic signals generated by eye
movements, 
characterised in
, further comprising the
steps of: processing said detected bio-electromagnetic

signals by assigning, in response to said detected bio-electromagnetic
signals, symbolic tokens corresponding to

eye movement classifications, thereby generating tokens
corresponding to said bio-electromagnetic signals;

generating command signals based on a protocol correlating
patterns of tokens to a desired command signal, and;

providing communication or control functions in response to
said command signals.
The method according to claim 23, 
characterised in

that said step of detecting includes detecting electro-oculograph
(EOG) signals.
The method according to claim 24, 
characterised in

said method further comprising the steps of: providing
digital data corresponding to sampled signals, said digital

samples being transmitted to a processor; and processing 
drift occurrence for providing an appropriate offset

voltage for substantially eliminating the effect of drift.
The method according to claim 24, 
characterised in

that said method during a calibration further comprises the
steps of: receiving EOG data resulting from a predetermined

eye movement; determining a set of one or more parameters
corresponding to a given eye movement classification;

wherein said eye movement classification corresponds to a
token from a set of defined tokens.
The method according to claim 24, 
characterised in

that said method during interaction further comprises the
steps of: receiving EOG data resulting from eye movements;

and generating a token from a set of defined tokens when
said EOG data derived from said eye movements falls within

a classified set of parameters corresponding to said
generated token.
The method according to claim 24, 
characterised in

said method further including the step of detecting signals
associated with head movements and processing said signals

in conjunction with EOG signals.
The method according to claim 24, 
characterised in

said method further including the step of receiving tokens
corresponding to each eye from EOG signal channels and

interpreting the tokens received from each eye, and further
interpreting combinations of tokens received from both

eyes.
The method according to claim 23, 
characterised in

said method including the steps of determining calibration
parameters for said eye movement classifications and

transforming said calibration parameters in accordance with
optical characteristics where said transformed calibration 

parameters are used for eye tracking with an optical device
having said optical characteristics.
The method according to claim 23, 
characterised in

said method further comprising the steps of: calibrating by
determining a ratio of actual distance to number of

detected tokens; detecting a given number of tokens
generated during an interaction mode; and multiplying said

given number of tokens by said ratio to determine an actual
distance.
The method according to claim 31, 
characterised in

that the step of determining said ratio includes a user
setting a sensitivity value.
The method according to claim 23, 
characterised in

said method further including the step of providing
numerical value tokens associated with said symbolic

tokens.
The method according to claim 33, 
characterised in

said method including a step of determining the distance of
an object by triangulation using said value tokens.
The method according to claim 23, 
characterised in

determining the 3D position and velocity of a gaze point
comprising the steps of: detecting signals associated with

eye movements from each eye; processing said detected
signals from each eye using triangulation to determine the

gaze point in 3D and the time rate of change of said gaze
point.
The method according to claim 35, 
characterised in

that said gaze point corresponds to an object that is
displayed on a screen (88) as a two dimensional

representation of a three dimensional space. 
The method according to claim 23 for disengaging an eye
tracking interface, 
characterised in
 that said method
further comprises the steps of: calibrating to determine a

normative fixated distance for a user's gaze at an object,
processing said detected bio-electromagnetic signals to

determine the distance of said user's fixated gaze during
an interaction period, and causing said eye tracking

interface to disengage when said distance differs from said
normative fixated distance.
The method according to claim 23 for EOG eye tracking or
control while a user is looking through an optical device,


characterised in
 that said method further comprises the
steps of: calibrating an EOG eye tracking system by a user

providing predetermined eye movements with reference to
calibrating points in said user's field of view as said

user looks through said optical device, and generating
parameter classifications based on said user provided eye

movements, so that during an interaction mode, actual eye
movements are detected and tokens corresponding to said

classifications are generated.
The method according to claim 38, 
characterised in

that said optical device is a microscopic device and said
calibrating points are on a calibrating plate.
The method according to claim 38, 
characterised in

that said optical device is a magnifying device and said
calibrating points appear on a lens of said magnifying

device.
The method according to claim 38 for electro-oculographic
(EOG) eye tracking or control while a user is looking

through an optical device 
characterised in
 said method
further comprising the steps of: calibrating an EOG eye

tracking system with the naked eye to determine naked eye 
calibration parameters; transforming said naked eye

calibration parameters using the optical characteristics of
said optical device; where said transformed calibration

parameters are used for eye tracking or control with said
optical device.
</CLAIMS>
</TEXT>
</DOC>
