<DOC>
<DOCNO>EP-0631250</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Method and apparatus for reconstructing three-dimensional objects
</INVENTION-TITLE>
<CLASSIFICATIONS>H04N1300	G06T1100	G06T1100	G06T700	H04N1300	G06T700	H04N1302	H04N1302	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>H04N	G06T	G06T	G06T	H04N	G06T	H04N	H04N	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>H04N13	G06T11	G06T11	G06T7	H04N13	G06T7	H04N13	H04N13	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
One of images of an object (19) captured by at least three 
cameras (111-11n) is defined as a basic image, and a basic 

backprojection line is defined which passes through a feature 
point in the basic image corresponding to a noted three-dimensional 

feature point on the object and the optical 

center of the camera having captured the basic image. 
Reference backprojection lines are calculated respective 

feature points on an epipolar line, obtained by projecting 
the basic backprojection line onto the image by each of the 

other cameras, and the optical center of said each camera. 
The coordinates of intersection points of the basic 

backprojection line and the reference backprojection lines 
are calculated and the number of the reference backprojection 

lines intersecting at each intersection point on the basic 
backprojection line is counted. The point having the maximum 

number of intersections is determined to be the position of 
the noted three-dimensional feature point. Similar 

processing is performed for all of the feature points in the 
basic image, by which the positions of corresponding three-dimensional 

feature points of the object are determined. 

</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
NIPPON TELEGRAPH 
&
 TELEPHONE
</APPLICANT-NAME>
<APPLICANT-NAME>
NIPPON TELEGRAPH AND TELEPHONE CORPORATION
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
MORI TAKEAKI
</INVENTOR-NAME>
<INVENTOR-NAME>
SUZUKI SATOSHI
</INVENTOR-NAME>
<INVENTOR-NAME>
YASUNO TAKAYUKI
</INVENTOR-NAME>
<INVENTOR-NAME>
MORI, TAKEAKI
</INVENTOR-NAME>
<INVENTOR-NAME>
SUZUKI, SATOSHI
</INVENTOR-NAME>
<INVENTOR-NAME>
YASUNO, TAKAYUKI
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The present invention relates to measurements of the
position, shape and movement of a three-dimensional moving
object and, more particularly, to a three-dimensional
information reconstruction or recovery method and apparatus
which can be used in the fields of three-dimensional
information reconstruction, recognition and description (CG)
of moving objects.Conventional three-dimensional information
reconstruction techniques can be classified into three
categories. A first technique is stereopsis. This technique
is one that establishes correspondence of points or lines
between two right and left images taken by two cameras and
estimates, from the positions of the cameras and pairs of
corresponding points or lines on the right and left images,
the positions of points and lines on a scene space
corresponding to the pairs of corresponding points or lines.
A second technique is a three-dimensional information
recovery method using a moving camera. This is a method
which tracks individual feature points on a number of images
picked up by a moving camera and estimates the positions of
points on a scene space corresponding to the feature points.
A third technique is a backprojection method, which recovers
or reconstructs a three-dimensional structure of an object in
a scene space by projecting back feature points in images to
the scene space.With the first technique (see, for example, Kanade T.,
Okutomi T. and Nakahara M., "A multiple baseline stereo
method," Proc. Image understanding Workshop, pp. 409-426,
1992 or United States Patent No. 4,654,872), many points on 
the surface of the object tend to be occluded from the sight
line of either one of the cameras because of uneven object
surface, and hence accurate positions of the corresponding
feature points between the right and left images cannot be
obtained, making it hard to obtain highly accurate three-dimensional
information. The second technique (see, for
example, Bolles R. C., Baker H. H. and Marimont D. H.:
"Epipolar-plane image analysis: an approach to determining
structure from motion," IJCV, Vol. 1, No. 1, pp. 7-55, 1987)
cannot be applied to a moving object, because the object
needs to stand still during its image capturing session by a
moving camera. Recently, there has been proposed a technique
which permits simultaneous extraction of the three-dimensional
shape and motion of an object from many images
taken by a single camera (see Tomasi C. and Tanade T.: "Shape
and motion from image streams under orthography: a
factorization method," IJCV, Vol.
</DESCRIPTION>
<CLAIMS>
A method of reconstructing three-dimensional information of an object (19) by
making three-dimensional feature points on said object to correspond to feature points in images

of said object captured by a plurality of cameras, said method comprising the steps:

(a) wherein said object is imaged by n cameras to obtain n frames of images and said
images are output as image data, said n being an integer equal to or greater than 3;
(b) wherein said image data is received and feature points of said object on said n
frames of images are respectively extracted;
(c) wherein one of said n cameras is defined as a basic camera, the image picked up by
said basic camera as a basic image (32s), the other remaining cameras as reference cameras, the

images captured by said reference cameras as reference images (32), and a backprojection line,
which passes through the optical center (31s) of said basic camera and a feature point in said

basic image chosen in correspondence to a noted three-dimensional feature point on said object,
is defined as a basic backprojection line;
(d) wherein said basic backprojection line is projected onto said reference images to
define thereon epipolar lines, respectively;
(e) wherein backprojection lines, which pass through said feature points on said epipolar
lines on said reference images and optical centers of said reference cameras, are defined as

reference backprojection lines;
(f) wherein the position of said noted threed-dimensional features point is determined from intersecting said basic backprojection line and said reference backprojection lines; and
(g) wherein said steps (c) through (f) are repeated for each of said feature points on said
basic image to obtain the position of said three-dimensional feature points on said object as its

three-dimensional information; said method being 
characterised in that
 for said determination
of said noted three-dimensional feature point coordinates of intersection points of said basic backprojection line and

said reference backprojection lines are calculated, the numbers of intersections of said basic
backprojection line and said reference backprojection lines at their respective intersection points

are counted, and the distribution of counts of said intersections along said basic backprojection
line is filtered by convolution to emphasize the concentration of said distribution of said

intersections, and the position where said filtered distribution of counts of said intersections is
maximum is determined to be the position of said noted three-dimensional feature point.
The method of claim 1, wherein a position of said basic backprojection line is defined
every predetermined unit length of said basic backprojection line lengthwise thereof and the

number of intersections at each of said intersection point is the number of intersections in said
unit length at the position on said basic backprojection line through which said reference

backprojection lines pass.
The method of claim 2, wherein said reference backprojection lines intersecting with
said basic backprojection line are reference backprojection lines lying within a predetermined

range of distance from said basic backprojection line.
The method of claim 1, wherein said filtering is performed by a Laplacian-Gaussian
filter. 
The method of claim 1, wherein said filtering is performed by a square filter.
The method of claim 1, wherein said filtering is performed by a triangular filter.
The method of claim 1, wherein said filtering is performed by a Gaussian filter.
The method of claim 1, wherein said step (g) includes a step wherein each time said
feature point corresponding to said noted three-dimensional feature point is identified by

performing said series of steps (c) through (f), said identified feature point is removed from
candidates for processing in the subsequent repetition of said steps.
The method of claim 1, wherein the number n of said cameras is in the range of from
5 to 8.
The method of claim 1, further comprising a step wherein upon each completion of
said step (g), said step (g) is newly repeated for every one of said cameras, using one of said

reference cameras as a new basic camera and said basic camera as a new reference camera.
The method of claim 1, further comprising a step wherein upon each repetition of
simultaneous capturing of images of said object (19) by said n cameras at a fixed period, three-dimensional

information of said object is obtained following said series of steps. 
An apparatus for reconstructing three-dimensional information of an object (19)
comprising:


n camera means (111,...,11n) disposed at different positions, for capturing images of an
object (19) and for outputting said images as image data, said n being an integer equal to or greater

than 3; and
image information processing means (20) supplied with said image data from said n camera
means, for acquiring three-dimensional information of said object from feature points in said images;

said image information processing means comprising:

feature extracting means (22) for extracting said feature points in said images;
basic backprojection line generating means (23) wherein one of said n camera means is
defined as a basic camera, the image captured by said basic camera means as a basic image (32s),

the other remaining camera means as reference camera means, the images captured by said
reference camera means as reference images (32), and whereby a backprojection line, which passes

through the optical center (31s) of said basic camera means and a feature point (33s) in said basic
image chosen in correspondence to a noted three-dimensional feature point on said object, is

obtained as a basic backprojection line (34s);
epipolar line generating means (24) for projecting said basic backprojection line onto said
reference images (32) to obtain epipolar lines (35);
reference backprojection line generating means (25) for obtaining, as reference
backprojection lines (34), backprojection lines which pass through feature points on said epipolar lines

(35) on said reference images and the optical centers of said reference camera means; and
three-dimensional feature extracting means (25-29) for determining the position of said noted
three-dimensional feature point from intersecting said basic backprojection line and said reference

backprojection lines;
said apparatus being 
characterized by
 said three-dimensional feature extraction means
calculating the coordinates of intersection points (36) of said basic backprojection line (34s) and said

reference backprojection lines (34), counting the number of intersections of said basic backprojection

line and said reference backprojection lines at each of said intersection points and determining the
position of that one of said intersection points which has the maximum count value to be the position

of said noted three-dimensional feature point;

   wherein said feature extracting means (22) includes filter means (28) for emphasizing the
concentration of said distribution of counts of intersections by convolution along said basic

backprojection line and the position where said filtered distribution of counts of intersections is
maximum is determined to be the position of said noted three-dimensional feature point;

   whereby the positions of respective three-dimensional feature points on said object,
corresponding to respective feature points on said basic image, are obtained as said three-dimensional

information of said object. 
The apparatus of claim 12, wherein said filter means (28) is a Laplacian-Gaussian
filter.
The apparatus of claim 12, wherein said filter means (28) is a square filter.
The apparatus of claim 12, wherein said filter means (28) is a triangular filter.
The apparatus of claim 12, wherein said filter means (28) is a Gaussian filter.
The apparatus of claim 12, wherein said number n of cameras means is in the range
of from 5 to 8.
The apparatus of claim 12, further comprising camera control means (14) for
repeating simultaneous image capturing of said object (19) by said n camera means at a fixed

period and for supplying said image data to said image information processing means (20) upon
each generation of said image data.
</CLAIMS>
</TEXT>
</DOC>
