<DOC>
<DOCNO>EP-0954856</DOCNO> 
<TEXT>
<INVENTION-TITLE>
CONTEXT DEPENDENT PHONEME NETWORKS FOR ENCODING SPEECH INFORMATION
</INVENTION-TITLE>
<CLASSIFICATIONS>G10L1526	G10L1518	G10L1500	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G10L	G10L	G10L	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G10L15	G10L15	G10L15	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A method and apparatus for generating a context dependent phoneme network as an intermediate step of encoding speech information. The context dependent phoneme network is generated from speech in a phoneme network generator (48) associated with an operating system (44). The context dependent phoneme network is then transmitted to a first application (52).
</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
MOTOROLA INC
</APPLICANT-NAME>
<APPLICANT-NAME>
MOTOROLA, INC.
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
AUSTIN STEPHEN
</INVENTOR-NAME>
<INVENTOR-NAME>
BALAKRISHNAN SREERAM
</INVENTOR-NAME>
<INVENTOR-NAME>
AUSTIN, STEPHEN
</INVENTOR-NAME>
<INVENTOR-NAME>
BALAKRISHNAN, SREERAM
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The present invention relates generally to computer speech
recognition.Recent advances in computer hardware and software have
allowed computer speech recognition (CSR) to cross the threshold
of usability. Systems are now available for high end personal
computers that can be used for large vocabulary, continuous speech
dictation. To obtain adequate performance, such systems need to be
adapted to a specific user's voice and environment of usage. In
addition, these systems can only recognize words drawn from a
certain vocabulary and are usually tied to a particular language
model, which captures the relative probabilities of different
sequences of words. Without all of these constraints, it is very
difficult to get adequate performance from a CSR system.In most CSR systems, the user and environment specific part,
or acoustic models, are usually separate to the vocabulary and
language models. However, because of the above constraints, any
application that requires speech recognition needs access to both
the user/environment specific acoustic models and the application
specific vocabulary and language models. This is a major obstacle to moving CSR systems beyond
standalone dictation, to systems where many different users will
need to access different applications, possibly in parallel and
often over the internet or a local area network (LAN). The reason
is that either: (a) each application will have to keep separate
acoustic models for each user/ environment; or (b) each user will
need to maintain separate sets of vocabularies and language models
for each application they wish to use. Since the size of acoustic
and language models are typically in the order of megabytes to
tens of megabytes for a medium to large vocabulary application, it
follows that in either scenario (a) or (b), the systems' resources
are going to be easily overwhelmed.One possibility is to store the acoustic models on a
different machine to the vocabulary and language models, and
connect the machines via a local area network (LAN) or the
internet. However, in either (a) or (b), enormous amounts of
network traffic will be generated as megabytes of data are shifted
to the target recognizer.A speech recognizer operating on a computer system
running several application programs at the same time
is disclosed in US-A-5651096.Thus, a need exists for a CSR system that is independent of
the vocabulary and language models of an application without
sacrificing performance in terms of final recognition accuracy.It is an object of the present
</DESCRIPTION>
<CLAIMS>
A method for encoding speech information comprising the steps of:

generating a context dependent phoneme network from speech in
a phoneme network generator associated with an operating system

using an acoustic model continuously adapting to a user's voice; and
transmitting the context dependent phoneme network to a first
application, wherein the phoneme network generator and the first

application are independently associated with the operating system.
The method according to claim 1 wherein the context dependent
phoneme network is vocabulary and language model independent.
The method according to claim 1 further comprising
extracting, at the first application, information needed from the

context dependent phoneme network using vocabulary and language
models of the first application in order to operate the first

application.
A method for encoding speech information comprising a step of providing
an operating system comprising two parts, wherein a first part of

the operating system is dependent on user and environment specific
acoustic models which are continuously adapting to a user's voice

and a second part of the operating system is dependent on a first
application selectable independently from the first part of the

operating system, wherein the second part of the operating system 
extracts information needed from the context dependent phoneme

network using vocabulary and language models of the first
application.
The method according to claim 4 wherein the first part of the
operating system generates a context dependent phoneme network, in

order to capture user and environment specific acoustic models and
transmits the context dependent phoneme network to the second part

of the operating system.
The method according to claim 4 further comprising a second
application, wherein the first application and the second

application comprise different vocabularies and different language
models.
The method according to claim 4 further comprising a second
application, wherein the first application and the second

application share a common vocabulary and a common language model. 
A method for encoding speech information comprising the steps of:

converting speech to a context dependent phoneme network in a first part of an
operating system with an aid of user and environment specific acoustic models and

phoneme language models;
adapting the context dependent phoneme network continuously based on the user
and environment specific acoustic models and phoneme language models;
extracting desired information required for speech recognition from a speech
signal;
packaging the desired information in a vocabulary, language model, user and
environment independent format; and
transmitting the desired information to a second part of the operating
system having a first application, wherein the context dependent phoneme

network and the first application are independently associated with the operating
system.
An apparatus for encoding speech information comprising:

an operating system;
a phoneme network generator generating a context dependent
phoneme network

by using an acoustic model continuously adapting to a user's voice
having an output for providing the context dependent

phoneme network; and
a first application adapted to receive the output of the
phoneme network generator and extract information needed from the

output using a vocabulary and a language model of the first
application, wherein the phoneme network generator and the first

application are independently associated with the operating
A data storage medium comprising instructions and data which,
when loaded into a first general purpose microprocessor having an

operating system, cause the first general purpose microprocessor to
comprise a phoneme network generator for generating a context

dependent phoneme network
by using an acoustic model continuously adapting to a user's voice

having an output for providing the context
dependent phoneme network and a first application adapted to

receive the output of the phoneme network generator and extract
information needed from the output using a vocabulary and language

model of the first application, wherein the phoneme network 
generator and the first application are independently associated

with the operating system.
The data storage medium according to claim 10 wherein the
data storage medium comprises a first part having stored thereon

the phoneme network generator and a second part having stored
thereo
n the first application. 
A method for encoding speech information comprising the steps of:

generating a context dependent phoneme network by using an acoustic model
continuously adapting to a user's voice in a first part of an operating system;
providing a second part of the operating system being dependent on a first
application selectable independently from the first part of the operating system;
transmitting the context dependent phoneme network to the second part of
the operating system which uses a first and second search engine to extract

from the context dependent phoneme network information relevant to the needs
of the first application using vocabulary and language models of the first

application;
attempting recognition in the first search engine;
upon unsatisfactory recognition in the first engine, forwarding the context
dependent phoneme network to a second search engine; and
retrieving from the second search engine a candidate recognition result which is
delivered to the first application.
</CLAIMS>
</TEXT>
</DOC>
