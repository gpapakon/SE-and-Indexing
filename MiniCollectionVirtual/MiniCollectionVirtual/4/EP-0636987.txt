<DOC>
<DOCNO>EP-0636987</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Functional pipelined cache memory.
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F1208	G06F1208	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F12	G06F12	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A virtual triple ported cache 16 operates as a true triple ported 
array by using a pipelined array design. Multiple execution units 12, 14 

can access the cache during the same cycle that the cache is updated from 
a main memory 19. The pipelined features of the cache allow for three 

separate sequential operations to occur within a single cycle 20, and 
thus give the appearance of a virtual triple ported array. This virtual 

triple port array architecture contains a data interface 17 for dual 
execution units, which allows both units to access the same data array 

location. The array architecture allows for back-to-back read accesses 
occurring within a half cycle. The array architecture provides a 

bypassing function around the array for a write occurring on one port to 
the same address that a read is occurring on the other port. To allow 

for simultaneous cache reloads during execution unit access, a late write 
is done at the end of the cycle. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
IBM
</APPLICANT-NAME>
<APPLICANT-NAME>
INTERNATIONAL BUSINESS MACHINES CORPORATION
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
BRACERAS GEORGE MARIA
</INVENTOR-NAME>
<INVENTOR-NAME>
HOWELL LAWRENCE CAREY JR
</INVENTOR-NAME>
<INVENTOR-NAME>
BRACERAS, GEORGE MARIA
</INVENTOR-NAME>
<INVENTOR-NAME>
HOWELL, LAWRENCE CAREY, JR.
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
This invention relates to the field of data processing systems, and 
more particularly to a cache memory allowing for concurrent access from 
multiple sources. Improving the performance of data processing systems is a never 
ending quest. One technique used to increase system performance is to 
perform certain operations in parallel, as opposed to serially. This 
allows for performing more than one operation at any given time. Some 
processors contain a plurality of execution units so that more than one 
instruction can be executed at a given time. Superscaler computers are 
an example of this type of architecture, where multiple instructions are 
simultaneously dispatched to multiple execution units for parallel 
execution. Caches are high speed memory arrays used to store instructions or 
data that are required by a processor or execution unit within the 
processor. The cache resides between the processor and slower, main 
memory. By maintaining instructions and/or data in the cache, the 
processor is able to access such instructions and/or data faster than 
access to the main memory. The use of multiple execution units presents a problem of how to 
allow each execution unit access to a single cache. Although providing 
separate caches to each execution would eliminate this problem, this 
solution increases system cost by requiring the addition of another cache 
array. This also results in added system complexity and redundancy, in 
that each execution unit is no longer sharing a common cache or executing 
a common instruction/data stream. Rather, the execution units are 
operating autonomously from one another. One approach to allowing multiple accesses to a cache in a system 
having two execution units is to design a cache array which is a dual 
port cache. Thus, each processor would have its own interface to the 
cache. However, there are distinct disadvantages with this approach,  
 
since a dual port array requires 30-50% more physical area than a single 
port array. The dual port array also has slower access times. Besides the processor interface requirements, another factor which 
must be considered in cache design is the interface to the main memory. 
A line of data, which consists of several cycles of data transfers, must 
be fetched from memory and written into the cache. However, this 
operation will interrupt accesses from the processor in either a single 
or dual port design. One approach to avoid interrupting the processor is 
to hold off one of the execution units until the reload is
</DESCRIPTION>
<CLAIMS>
A method for operating a cache (16) to interface a memory (19) with 
a plurality of data processors (12, 14), said cache providing concurrent 

operations to said memory and said plurality of processors, comprising 
the steps of: 

   accessing said cache by a first processor (12) and second 
processor (14) during a first portion (22) of a clock cycle (20); 

and 
   accessing said cache by said memory during a second portion 

(24) of said clock cycle. 
A method for operating a cache (16) to interface a memory (19) to a 
plurality of data processors (12, 14), said cache providing concurrent 

operations to said memory and said plurality of processors, comprising 
the steps of: 

   accessing said cache by a first processor (12) during a first 
portion (26) of a clock cycle (20); 

   accessing said cache by a second processor (14) during a 
second portion (28) of said clock cycle; and 

   accessing said cache by said memory during a third portion 
(30) of said clock cycle. 
A method as claimed in any preceding claim wherein said clock cycle 
(20) is comprised of a first half cycle (22) and a second half cycle 

(24), and wherein said first (26) and second (28) portions occur within 
said first half cycle and said third portion (30) occurs within said 

second half cycle. 
A cache (16) for interfacing a memory (19) with a plurality of data 
processors (12, 14), said cache providing concurrent operations to said 

memory and said plurality of processors, comprising:
 

   means for accessing said cache by a first processor (12) and 
second processor (14) during a first portion (22) of a clock cycle 

(20); and 
   means for accessing said cache by said memory during a second 

portion of said clock cycle. 
A cache (16) for interfacing a memory (19) to a plurality of data 

processors (12, 14), said cache providing concurrent operations to said 
memory and said plurality of processors, comprising: 

   means for accessing said cache by a first processor (12) 
during a first portion (26) of a clock cycle (20); 

   means for accessing said cache by a second processor (14) 
during a second portion (28) of said clock cycle; and 

   means for accessing said cache by said memory during a third 
portion (30) of said clock cycle. 
</CLAIMS>
</TEXT>
</DOC>
