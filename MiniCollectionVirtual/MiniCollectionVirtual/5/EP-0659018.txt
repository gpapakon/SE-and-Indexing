<DOC>
<DOCNO>EP-0659018</DOCNO> 
<TEXT>
<INVENTION-TITLE>
An animated electronic meeting place
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F1500	H04N700	G06T1740	H04N700	G06T1740	H04N715	H04N715	G06F1500	G09G500	G09G500	H04N726	H04N726	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	H04N	G06T	H04N	G06T	H04N	H04N	G06F	G09G	G09G	H04N	H04N	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F15	H04N7	G06T17	H04N7	G06T17	H04N7	H04N7	G06F15	G09G5	G09G5	H04N7	H04N7	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A network-based animated electronic meeting place is 

provided for business meetings, education, simulated 
classrooms, casual encounters, personal meetings, art 

auctions, parties and game playing, which includes a 
terminal for each participant on the network and local 

prestored animation graphics, with simulated participants 
in the form of animated characters or dolls being driven 

through sensing of each of the participants at their 
respective work stations, including their position, 

posture, gestures, audio content and optionally that 
persona which each individual wishes to be displayed at 

the animated electronic meeting place. In one 
embodiment, a user chooses how he represents himself on 

the screen through the use of a Persona or Character 
control. The animated electronic meeting place is 

provided with real time 3-D graphics renderings, showing 
the meeting with all of the participants including the 

individual participant as himself, a microphone to 
capture the use's speech, digital sound processing for 

voice localization, and position sensors to detect the 
participant's gestures and/or facial expressions as well 

as body movement. In one embodiment, the user is also 
provided with a view control in the form of a joy stick 

to zoom in or to alter the perspective at which he is  
 

viewing the animated meeting. In a further embodiment, 
through sound localization detection as well as speech 

recognition circuits, stereo sound at each terminal can 
be steered so as to localize the sound to the person who 

is detected as talking. 

</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
MITSUBISHI ELECTRIC CORP
</APPLICANT-NAME>
<APPLICANT-NAME>
MITSUBISHI DENKI KABUSHIKI KAISHA
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
TOHEI NITTA
</INVENTOR-NAME>
<INVENTOR-NAME>
TOHEI, NITTA
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
This invention relates to an electronic meeting
place and more particularly to a meeting place in which
animated characters are utilized to represent
participants.Various video conferencing techniques have been
utilized in the past to provide full frame video of
participants for the video conference, most usually with
each of the participants represented at a window on the
terminal or display device utilized. What all of the
prior video conference systems have in common is that it
is necessary to provide full frame video which is band
width intensive and, inter alia, does not permit the
participants to alter their image which is displayed to
other participants. One such video conferencing system
by K. Hiratsuka and H. Kakihara entitled Video
Conferencing System Japan Telecommunications Review,
Vol.18, No. 3, pp. 145-151, July 1976, utilizes full
frame video and accommodates large numbers of conferees
by dividing up each scene into different segments with
each segment filmed by different cameras. The purpose of
dividing up the scene into segments is to be able to 
portray conferees large enough to be recognized, noting
that if all the conferees were pictured, the faces would
be too small to be recognized. A similar system is
illustrated in U.S. Patent 3,601,530 issued to Robert C.
Edson et al., illustrating a video conferencing system
using voice switched cameras, with the cameras dividing
up the scene. Note that a video conferencing system
corresponding to the aforementioned Japanese system is
also published in the Proceedings of the IEEE, Vol. 73,
No. 4, April 1985, authored by Hakar Sabri and Birendra
Prasada.As illustrated in U.S. Patent, 4,004,084 issued to
Earl Franklin Brown et al., a video conferencing system
utilizes spatial reduction and temporal resolution to
minimize bandwidth requirements for full-frame video
utilizing speech to select which picture is automatically
transmitted. Here only one picture is displayed at a
time.British Patent 1,173,918 illustrates the utilization
of a TV screen to present full frame video corresponding
to the various participants in separate parts of the TV
screen.As illustrated in U.S. Patent 5,1517,491, modular
screen approaches have been utilized in video
teleconferencing, again to provide a full likeness of the 
individual at various modules. Additionally, U.S. Patent
No. 4,965,819 to Dino Kannes describes a video
conferencing system for courtroom applications in which
full-framed video likenesses of participants are
presented at various portions of the
</DESCRIPTION>
<CLAIMS>
A video conferencing system for an animated electronic meeting place in which
characters representing participants (12, 14, 16) from remote locations are depicted in a

scene (32), characterized by

means at a first location for transmitting audio
signals corresponding to speech of a participant and

animation graphic control signals corresponding to the
position and motion that the character corresponding to

said participant is to possess in said scene (32); and,
means (50) at a second location and responsive to said
signals for reproducing said speech and for generating

said scene with each of said participants (12, 14, 16) therein in
animated form as a character driven by said animation

graphic control signals.
The video conferencing system of Claim 1, and
further including means at said first location for

reproducing at least a part of said scene (32).
The video conferencing system of Claim 1, or
2, wherein said scene (32) at said first location includes the

character corresponding to the participant (12) thereat so
that said participant (12) can view himself as he participates

in the meeting.
The video conferencing system of Claim 1, 2,
or 3, wherein said means for transmitting said animation

graphic control signals includes means (20) for controlling 
the persona represented thereby such that said

participant (12) can control how he is represented by his
character in said scene.
The video conferencing system of Claim 4,
wherein said persona control means (20) includes means (74) for

altering said animation graphic control signals such that
the corresponding character in said scene conveys a mood

controlled by said participant (12).
The video conferencing system of Claim 1, 2,
3, 4, or 5, wherein said means for transmitting said

animation graphic control signals includes means (82) for
controlling the position in said scene (32) of the character

corresponding to said participant (12).
The video conferencing system of Claim 1, 2,
3, 4, 5, or 6, wherein said means for transmitting said

animated graphic control signal includes means (84) for
controlling the gaze of the character corresponding to

said participant (12).
The video conferencing system of Claim 1, 2,
3, 4, 5, 6, or 7, wherein said means for generating said

scene (32) includes means for steering the apparent direction
of the reproduced speech to the location of a speaking

character.
The video conferencing system of Claim 1, 2,
3, 4, 5, 6, 7, or 8, wherein said means for generating 

said scene (32) includes means (78) for prestoring animation images
and graphics as a data base, and an animation model (76)

coupled to said data base for generating the animated
characters in said scene (32), thereby to preclude the

necessity of transmitting full frame bandwidth intensive
video from said first location to said second location.
The video conferencing system of Claim 9,
wherein said animation model (76) is a 3-D model for

generating a 3-D scene.
The video conferencing system of Claim 10,
or 11, and further including means (34) at said second

location for altering the perspective of said 3-D scene
to permit viewing said scene (32) at differing vantage points.
The video conferencing system of Claim 10, or
11, and further including means for zooming in and out of

said 3-D scene.
The video conferencing system of Claim 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 11, or 12, and further including

means (22, 20) at said first location for detecting the position
and orientation of at least a portion of said participant

and for altering said animation graphic control signals
responsive thereto, such that the character corresponding

to said participant is like positioned and oriented.
The video conferencing system of Claim 4, 5,
6, 7, 8, 9, 10, 11, 12, or 13, wherein said persona 

control means (20) includes means for recognizing gestures of
said participant (12) and for altering said animation graphic

control signals responsive thereto, such that the
character corresponding to said participant (12) is shown with

corresponding gestures.
The video conferencing system of Claim 14,
wherein said means (20) for recognizing gestures includes

means for recognizing a gesture representing a
predetermined mood and wherein said animated graphic

control signal altering means includes means (74) for altering
said signals such that the character representing said

participant (12) exhibits said predetermined mood.
A video conferencing method for a
conference system using an animated electronic meeting place in which characters representing

participants (12, 14, 16) from remote locations are depicted in a scene (32), characterised by the
steps of:


generating, at a first terminal, audio signals
corresponding to speech of a participant and animation

graphic control signals corresponding to the position and
motion that the character corresponding to said 

participant is to possess in a scene,
transmitting the audio signals and the animation
graphic control signals from the first terminal to a

second terminal;
receiving the audio signals and the animation
graphic control signals from the first terminal to the

second terminal; and
reproducing said speech from said audio signal and
generating said scene with each of said participant in

animated form as a character driven by said animation
graphic control signals.
The video conferencing method of Claim 16, wherein
said animation graphic control signals generating step

includes the steps of controlling an appearance of said
character and altering said animation graphic control

signals based on the appearance controlled by controlling
step.
The video conferencing method of Claim 16, wherein
said animation graphic control signal generating step

includes the step of optionally including the character
corresponding to the participant thereat in said scene.
</CLAIMS>
</TEXT>
</DOC>
