<DOC>
<DOCNO>EP-0637799</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Shared cache for multiprocessor system.
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F1208	G06F1208	G06F1516	G06F15167	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G06F	G06F	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F12	G06F12	G06F15	G06F15	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A high performance shared cache 100 is provided to support 
multiprocessor systems and allow maximum parallelism in accessing the 

cache by the processors 3, servicing one processor request in each 
machine cycle, reducing system response time and increasing system 

throughput. The shared cache of the present invention uses the 
additional performance optimization techniques of pipelining cache 

operations (loads and stores) and burst-mode data accesses. By including 
built-in pipeline stages, the cache is enabled to service one request 

every machine cycle from any processing element. This contributes to 
reduction in the system response time as well as the throughput. With 

regard to the burst-mode data accesses, the widest possible data out of 
the cache can be stored to, and retrieved from, the cache by one cache 

access operation. One portion of the data is held in logic in the cache 
(on the chip), while another portion (corresponding to the system bus 

width) gets transferred to the requesting element (processor or memory) 
in one cycle. The held portion of the data can then be transferred in 

the following machine cycle. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
IBM
</APPLICANT-NAME>
<APPLICANT-NAME>
INTERNATIONAL BUSINESS MACHINES CORPORATION
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
DIBRINO MICHAEL THOMAS
</INVENTOR-NAME>
<INVENTOR-NAME>
HICKS DWAIN ALAN
</INVENTOR-NAME>
<INVENTOR-NAME>
LATTIMORE GEORGE MCNEIL
</INVENTOR-NAME>
<INVENTOR-NAME>
SO KIMMING K
</INVENTOR-NAME>
<INVENTOR-NAME>
YOUSSEF HANAA
</INVENTOR-NAME>
<INVENTOR-NAME>
DIBRINO, MICHAEL THOMAS
</INVENTOR-NAME>
<INVENTOR-NAME>
HICKS, DWAIN ALAN
</INVENTOR-NAME>
<INVENTOR-NAME>
LATTIMORE, GEORGE MCNEIL
</INVENTOR-NAME>
<INVENTOR-NAME>
SO, KIMMING K.
</INVENTOR-NAME>
<INVENTOR-NAME>
YOUSSEF, HANAA
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The present invention relates to a cache for use in a 
multiprocessor (MP) computer system. More particularly, a cache is 
provided that is capable of accommodating one data request (load or 
store) per processor each cycle. In conventional multiprocessor systems it is common to design a 
larger cache than commonly used in uniprocessor systems. This is due to 
the fact that most program applications in multiprocessor systems require 
more data manipulations than uniprocessor applications. The problem with 
larger caches is that more signal propagation delay time is present and 
more levels of logic are required to decode data addresses, which causes 
the cache access time to increase due to the large cache size. Thus, 
cache designers are faced with the challenge of trying to satisfy the 
needs of the multiprocessor systems' application by providing sufficient 
cache density, while maintaining optimal system performance. The IBM Technical Disclosure Bulletin, volume 34, number 1, June 
1991, describes a memory hierarchy for a multiprocessor system in which 
each processor has a private level-one (L1) cache and a level-two (L2) 
cache shared by plural number of the processors. When a line is loaded 
with data from the L2 cache, to be provided to the L1 cache, the location 
in the L1 cache is recorded such that the location ca be used to access 
the L2 cache for subsequent store operations, without having to look up 
the L2 directory. U.S. patent number 4,371,929 discusses a multiprocessor system with 
a controllable cache store interface to a memory which employs a 
plurality of storage partitions having interleaved access in a time 
domain multiplexed manner on a common bus. The storage partitions are 
uniquely associated with each host adapter, corresponding to each 
processor. Interleaved operations allow several host processors to be 
serviced during a single host processor I/O channel transfer period. 
However, when a full block data transfers from the cache to memory is 
started, interleaving of other data transfers with the full block  
 
transfer are not permitted. Thus, certain data transfers must wait until 
the full block transfer is complete. U.S. patent number 4,056,845 describes a cache memory system which 
can be used for interleaved or non-interleaved operation. U.S. patent 
number 4,445,174 discusses a multiprocessing system wherein each 
processor has a private cache and shares a common cache and main memory 
with the other processors. U.S. patent number 4,905,141 describes a 
cache
</DESCRIPTION>
<CLAIMS>
A data processing system having a plurality of processing units 
(3), comprising: 

   a cache (100) for storing data to be utilized by said processing 
units; 

   a system bus (56) for transferring data between said cache and said 
processing units; and 

   circuit means (100) for transferring, in a single operation, data 
into and out of said cache, in an amount greater than a capacity of said 

system bus. 
A system as claimed in claim 1 further comprising pipeline means 
for latching data which is being stored into and retrieved from said 

cache. 
A system as claimed in claim 1 wherein said cache comprises plural 
interleaved sections each with input and output ports such that said 

plurality of processing units can simultaneously access said cache. 
A system as claimed in claim 1 wherein said circuit means further 
comprises: 

   means (110) for inputting data from said processing units to said 
cache; and 

   means (150) for outputting data from said cache to said processing 
units. 
A system as claimed in claim 4 wherein said input means comprises: 
   a store queue (127) for sequentially storing data words to be input 

from said processing element to said cache; 
   a multiplexer (128) for combining adjacent ones of said 

sequentially stored data words; 
   means for providing said combined data word to said cache; and 

wherein said output means comprises: 
   a multiplexer (201) and a latch (203) for separating said combined 

data word into plural data words; and 
   a register (207) and a driver (209) for placing said plural data 

words on said system bus. 
A system as claimed in claim 5, further comprising arbitration 
means (300) for awarding access to one of said interleaves of said cache 

to the processing element having the least recent access to said one 
interleaf, when more than one of said processing elements are 

concurrently requesting access to the one said interleaf. 
A system as claimed in claim 4, further comprising means for 
directly transferring data between plural ones of said processing 

elements, the means for directly transferring data comprising means for 
providing said data directly from said means for inputting to said means 

for outputting, such that the data bypasses said cache. 
A method of storing data in data processing system having a 
plurality of processing units, said method comprising the steps of: 

   storing data to be utilized by said processing units in a cache; 
   providing data between said cache and said processing units on a 

system bus; and 
   transferring, in a single operation, data into and out of said 

cache, in an amount greater than a capacity of said system bus. 
A method as claimed in claim 8 further comprising the step of 
latching data which is being stored into and retrieved from said cache; 

wherein said step of storing comprises the step of simultaneously 
accessing said cache by said plurality of processing units through plural 

interleaved sections, each with input and output ports. 
A method as claimed in claim 8 wherein said step of transferring 
further comprises the steps of: 

   inputting data from said processing units to said cache by: 
      sequentially storing data words to be input from said 

processing element to said cache; 
      combining adjacent ones of said sequentially stored data 

words; and 
      providing said combined data word to said cache; 

   outputting data from said cache to said processing units by: 
      separating said combined data word into plural data words; 

and 
      placing said plural data words on said system bus. 
</CLAIMS>
</TEXT>
</DOC>
