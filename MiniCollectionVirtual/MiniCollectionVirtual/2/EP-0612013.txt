<DOC>
<DOCNO>EP-0612013</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Combination prefetch buffer and instruction cache
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F938	G06F938	G06F1208	G06F1208	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G06F	G06F	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F9	G06F9	G06F12	G06F12	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A cache memory system features a combination 
instruction cache and prefetch buffer, which obviates 

any requirement for a bus interconnecting the cache 
and buffer and which also effectively allows the 

instruction buffer to write data into the cache with 
improved utilization of prefetched instructions and 

with decreased use of power and silicon space. 

</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
ADVANCED MICRO DEVICES INC
</APPLICANT-NAME>
<APPLICANT-NAME>
ADVANCED MICRO DEVICES, INC.
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
TRAN THANG
</INVENTOR-NAME>
<INVENTOR-NAME>
TRAN, THANG
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
This application is related to the following patent 
applications: 
US SERIAL NO TITLE EP APPLICATION NO 08006731Circuit for Delaying Data Latching from a Precharged Bus and Method(our reference HRW/TT0167/BEP) 08/006744Data Cache Reloading System and Method(our reference HRW/TT0168/BEP) 08/007073Dram Access System and Method(our reference HRW/TT0169/BEP) Our above listed European Patent applications are all 
filed on the same date as this application and are all 
hereby incorporated herein by reference as if reproduced in 
their entirety herein.  The present invention relates to electronic 
digital data processing systems such as 
electronic digital data processing 
systems which include a cache memory as well as a 
main memory. Improvements in data processing systems have 
generally been directed towards reduction of either 
the average time required to execute a given 
instruction or the cost of the equipment required to 
execute such an instruction. One design tradeoff 
which has typically been made is that of cost versus 
speed for units of memory for the storage of data. 
For example, tape memory is traditionally slower and 
less expensive than disk memory. Disk memory in turn 
is available in several types with the selection of 
any one type over another involving a cost/speed 
tradeoff. Disk memory is slower but less expensive 
than solid-state memory which itself is available in 
several types, the selection of which again involves 
a cost/speed tradeoff. Thus, it continues to be a 
need of the art to provide cheaper, faster memories 
or, failing that, to improve the efficiency of 
presently existing memory types. The present 
invention relates to an improvement of the second 
type. In particular, we will describe 
apparatus and methods of operation for reducing the 
average time necessary for a host central processing 
unit (CPU) having an associated cache memory and a 
main memory to obtain stored data from either memory. By way of background, it should be appreciated 
that computer systems are generally provided with 
more than one type of memory. Recognizing that the  
 
cost of a single fast memory would be prohibitive, 
computer designers have heretofore employed a variety 
of devices to hold data and instructions, the 
repository for each piece of information being 
selected based upon how urgently the information 
might be needed by the CPU. That is, in general, 
fast but expensive memories are used to store 
information the CPU might need immediately, and 
slower but less expensive
</DESCRIPTION>
<CLAIMS>
A cache memory system comprising: 
   a cache, said cache formed of a plurality of 

blocks with a plurality of bits per each block; and 
   a prefetch buffer, said prefetch buffer formed 

of a plurality of blocks equal in number to said 
number of blocks forming said cache, and said 

prefetch buffer directly operationally connected to 
said cache, 

   whereby use of a bus interconnecting said cache 
and said prefetch buffer is not required. 
A cache memory system as recited in claim 1, 
wherein said prefetch buffer comprises two separate 

valid-bit sets. 
A cache memory system as recited in claim 2, 
wherein one of said two separate valid-bit sets is 

used to write said instruction cache. 
A cache memory as recited in claim 3, wherein 
said cache memory system interacts with an external 

decoding section, and wherein said other of said two 
separate valid-bit sets is used to send instructions 

to said external decoding section. 
A cache memory system as recited in claim 1, 
wherein said prefetch buffer comprises a write 

counter and a read counter. 
A cache memory system as recited in claim 5, 
wherein said prefetch buffer is configured to operate 

in a first-in, first-out manner, which configuration 
inherently defines a head portion and a tail portion 

thereof. 
A cache memory system as recited in claim 6, 
wherein said write counter points to said head 

portion of said prefetch buffer. 
A cache memory system as recited in claim 7, 
wherein said read counter points to said tail portion 

of said prefetch buffer. 
A cache memory system as recited in claim 5, 
wherein said prefetch buffer further comprises a set 

of valid bits operable to indicate valid instructions 
therein. 
A cache memory system as recited in claim 9, 
wherein said prefetch buffer still further comprises 

a second set of valid bits operable to indicate 
whether instructions have updated said cache. 
A method for manipulating instructions in a 
microprocessor comprising the steps of: 

   determining that an instruction is required; 
   determining whether said required instruction is 

in the instruction cache portion of an instruction 
cache and prefetch buffer system; 

   fetching said required instruction from said 
instruction cache portion if therein; 

   fetching said required instruction from external 
memory if not in said instruction cache portion; 

   receiving said fetched instruction from external 
memory in the prefetch buffer portion of said 

instruction cache and prefetch buffer system; and 
   sending simultaneously said fetched instruction 

to said instruction cache portion and to a processing 
unit portion of said microprocessor for execution. 
A method as recited in claim 11, further 
comprising the step of prefetching subsequent blocks 

of instruction in a chain of ordered blocks so that 
subsequent blocks are readily available as needed 

within said instruction cache and prefetch buffer 
system. 
A method as recited in claim 11, further 
comprising the steps of determining whether 

instructions in said prefetch buffer are valid and so 
indicating. 
A method as recited in claim 13, still further 
comprising the steps of determining whether 

instructions have updated the instruction cache 
portion and so indicating. 
</CLAIMS>
</TEXT>
</DOC>
