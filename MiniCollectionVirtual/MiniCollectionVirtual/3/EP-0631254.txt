<DOC>
<DOCNO>EP-0631254</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Neural network and method of using same.
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F748	G06F7552	G06F1518	G06F1518	G06N300	G06N304	G06N3063	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G06F	G06F	G06N	G06N	G06N	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F7	G06F7	G06F15	G06F15	G06N3	G06N3	G06N3	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A neural network (FIG. 4), which may be implemented either 
in hardware or software, is constructed of neurons or neuron 

circuits (55, 56, 58) each having only one significant processing 
element in the form of a multiplier. The neural network utilizes a 

training algorithm which does not require repetitive training and 
which yields a global minimum to each given set of input vectors. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
MOTOROLA INC
</APPLICANT-NAME>
<APPLICANT-NAME>
MOTOROLA INC
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
WANG SHAY-PING THOMAS
</INVENTOR-NAME>
<INVENTOR-NAME>
WANG SHAY-PING THOMAS
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The present invention is related to the following invention 
which is assigned to the same assignee as the present invention: "Artificial Neuron and Method of Using Same", having United 
States of America patent application number 08/076,602, filed 
concurrently herewith. The subject matter of the above-identified related invention 
is hereby incorporated by reference into the disclosure of this 
invention. This invention relates generally to artificial neural 
networks and, in particular, to a neural network that can be 
implemented in a VLSI (very large scale integration) chip or in a 
computer program and which utilizes a training algorithm that 
does not require repetitive training and that yields a global 
minimum to each given set of input vectors. Artificial neural networks have utility in a wide variety of 
computing environments, such as speech recognition, process 
control, optical character recognition, signal processing, and 
image processing. Processing engines for many of the foregoing 
may be implemented through neural networks comprising a 
plurality of elemental logic elements called neuron circuits. A neuron circuit (or processing element) is the fundamental 
building block of a neural network. A neuron circuit has multiple 
inputs and one output. As described in the Related Invention identified above, the 
structure of a conventional neuron circuit often includes a 
multiplier circuit, a summing circuit, a circuit for performing a 
non-linear function (such as a binary threshold or sigmoid  
 
function), and circuitry functioning as synapses or weighted input 
connections. Because a typical conventional neuron circuit 
requires all of the above-described circuitry, the number of 
neuron circuits which can be manufactured on a semiconductor 
chip is severely limited. There are more than twenty known types of neural network 
architectures, of which the "back-propagation", "perceptron", and 
"Hopfield network" are the best known. FIG. 1 shows a prior art back-propagation neural network. 
As shown in FIG. 1, the back-propagation network typically 
comprises at least three layers: an "input layer", a "hidden layer", 
and an "output layer". However, as is well known, many more than 
three layers may be required to solve medium-sized problems. With reference to the specific back-propagation neural 
network shown in FIG. 1, each of a plurality of inputs x₁-xn is 
coupled to a respective input node in the input layer (of which only 
input nodes 1, 2, and 4 are shown). For example, input x₁ is

</DESCRIPTION>
<CLAIMS>
A neural network (FIG. 4) having a plurality of network 
inputs (xn) and at least one network output (y), said neural 

network comprising: 
   a plurality of neurons (55, 56, 58), each neuron having a 

plurality of inputs responsive to corresponding ones of said 
plurality of network inputs and generating an output; and 

   means (60) for summing the outputs of said neurons and 
generating said at least one network output. 
The neural network recited in claim 1 wherein said 
neurons comprise a linear transfer function. 
The neural network recited in claim 1 wherein said 
neural network is contained on at least one integrated circuit. 
The neural network recited in claim 1 wherein said 
neural network is contained in a computer program. 
A method for training a neural network (FIG. 11) 
comprising a plurality of neurons, which method requires the 

calculation of weight values and which method does not require 
repetitive training, said method comprising the following steps: 


(a) providing a plurality of training examples (121); 
(b) comparing the number of said training examples with the 
number of neurons in said neural network (122); 
(c) providing at least two techniques for calculating said 
weight values, wherein 


(i) if the number of neurons is equal to the number 
of training examples, using a matrix-inversion 

technique to solve for the value of each weight 
(123, 125); 
(ii) if the number of neurons is not equal to the 
number of training examples, using a least-squares 

 
estimation technique to solve for the 

value of each weight (123, 126). 
The method of training a neural network recited in 
claim 5 wherein the operation of said neural network is based 

upon a polynomial expansion. 
The method of training a neural network recited in 
claim 6 wherein said polynomial expansion has the form: 

 
   wherein y represents the output of the neural network; 

   wherein wi-1 represents the weight value for the ith neuron; 
   wherein x₁, x₂, . . ., xn represent inputs to said neural 

network; 
   wherein g1i, . . ., gni represent gating functions for the ith 

neuron which are applied to said inputs; and 
   wherein n is a positive integer. 
The method of training a neural network recited in 
claim 7 wherein each xi is represented by the function xi = fi(zj), 

wherein zj is any arbitrary variable, and wherein the indices i and 
j are any positive integers. 
In a neural network comprising: 
   a plurality of network inputs and at least one network 

output; 
   a plurality of neurons, each neuron receiving a plurality of 

inputs and generating an output; 
   a method of operating said neural network (FIG. 7), said 

method comprising the following steps: 

(a) distributing each of said network inputs to each of 
said neuron inputs (81); 
(b) each neuron applying a gating function to each of said 
network inputs to produce corresponding gated inputs (82); 
(c) each neuron multiplying each of said gated inputs 
together to generate a product (83); 
(d) each neuron multiplying said product by a weight value 
to generate a neuron output for each neuron (84); and 
(e) summing the outputs of said neurons to generate said 
at least one network output (85). 
The method of operating a neural network recited in 
claim 9 wherein the operation of said neural network is based 

upon a polynomial expansion. 
</CLAIMS>
</TEXT>
</DOC>
