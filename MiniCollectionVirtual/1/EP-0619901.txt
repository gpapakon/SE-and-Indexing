<DOC>
<DOCNO>EP-0619901</DOCNO> 
<TEXT>
<INVENTION-TITLE>
RAPIDLY CONVERGING PROJECTIVE NEURAL NETWORK.
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F1518	G06F1518	G06K962	G06K962	G06N300	G06N304	G06N308	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G06K	G06K	G06N	G06N	G06N	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F15	G06F15	G06K9	G06K9	G06N3	G06N3	G06N3	</CLASSIFICATIONS-FOURTH>
<APPLICANTS>
<APPLICANT-NAME>
R 
&
 D ASS
</APPLICANT-NAME>
<APPLICANT-NAME>
R 
&
 D ASSOCIATES
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
MANUKIAN NARBIK
</INVENTOR-NAME>
<INVENTOR-NAME>
WILENSKY GREGG D
</INVENTOR-NAME>
<INVENTOR-NAME>
MANUKIAN, NARBIK 440 MYRTLE STREET
</INVENTOR-NAME>
<INVENTOR-NAME>
WILENSKY, GREGG, D.
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
 RAPIDLY CONVERGING PROJECTIVE NEURAL NETWORKTechnical FieldThis invention relates to a system and a method for a neural network in which the number of elements representing an input pattern or function is deliberately increased so that the input pattern or function is represented and evaluated as a projection onto a normalized space of higher dimension.Background of the Invention A large class of problems, such as speech recognition, handwriting recognition, robotic control, function fitting and others, is difficult to solve or remain unsolved with conventional computing methods. They can, however, be cast in the form of pattern classification or optimization problems for which neural network methods have shown promise of solution.A neural network is a type of computer or processor structure that resembles the structure of the human brain, in that data is processed in a multi-layered system of interconnected "nodes" or "neurons," each of which might be a set of memory cells or even a group of individual processors.Conventional computers are programmed in a more or less completely non-adaptive manner, so that their ability to recognize patterns or a common structure in a data input stream is exclusively dependent on how expansive, detailed, and predictive its program is. In contrast, a neural network begins with interconnected nodes with biases, and it develops its own program through "training." Training normally involves presenting the network with a large number of training patterns with known values. The network's output is evaluated, and "mistakes" cause the network to adjust its internal parameters and interconnections in order to improve its 

 performance. In other words, the network "learns," and its performance typically will improve as it is "trained." As an example, assume a neural network is to be trained to distinguish between digitized images representing the "A" and "B." In this case, the network has two outputs, namely, "A" and "B." When the network is presented with an image of an "A," it is to recognize this and activate the output "A." In order to train the network, a stream of "A" and "B" images is input to the network. For each input symbol, the network analyzes the input data and indicates whether it received an "A" or a »»B." Every time the network decides that the input "A" is a "B," it is told that it has made a mistake, and it can then adjust the values of the neural connections and biases so that it will reduce the probability that it will make the same mistake again. In
</DESCRIPTION>
<CLAIMS>
We claim:
1. A data processing system comprising: a) input means for representing each of a series of input data groups as a sequence of N numerical values to form a corresponding N-dimensional base input vector, and for storing each base input vector; b) neural network means including: i) input layer storage means, intermediate layer storage means and output layer storage means; ii) said input layer storage means comprising at least N+j projected input memory units, where j is a predetermined positive integer, for storing a normalized projected input vector having N+j numerical elements, with each projected input vector corresponding to one of the base input vectors; iii) said intermediate layer storage means comprising a plurality of intermediate memory units for storing intermediate network values; iv) said output layer storage means comprising a network output node for storing a network output value; v) connection means for connecting each projected input memory unit with predetermined ones of the intermediate memory units and for connecting the output node with predetermined ones of intermediate memory units; c) processor and control means: i) for augmenting each N-dimensional base input vector with j projection elements to form said projected input vector; ii) for each intermediate memory unit in a lowest intermediate layer, for computing an intermediate threshold value and an intermediate weight vector, with each weight vector having N+j weight elements; and iii) for computing an output value as a predetermined function of the intermediate weight vectors, the intermediate threshold values and the projected input vectors. 


 2. A system as defined in claim 1, in which the processor and control means is further provided for normalizing the elements of the projected input vector so that the magnitude of the projected input vector is equal to a predetermined input normalization value; and for normalizing the elements of the intermediate weight vectors so that the magnitude of each intermediate weight vector is equal to a predetermined weight normalization value.
3. A system as defined in claim 2, including N+j weight elements for each weight vector in the lowest layer.
4. A system according to claim 1, further including: comparison means for comparing the network output value with predetermined goal vectors; and in which the processor and control means is further provided for recomputing the intermediate threshold values and intermediate weight vectors until the network output values differ by less than a predetermined minimum threshold amount from the corresponding goal vectors.
5. A system as defined in claim 1 which further includes a plurality of output nodes, in which the processor and control means is further provided for computing an output threshold value and an output weight vector for each output node and in which the processor and control means is further provided for recomputing the intermediate and output threshold values and intermediate and output weight vectors until the network output values differ by less than the predetermined minimum threshold from the corresponding goal vectors.
6. A system as defined in claim 1, in which the input means includes pattern resolution means for 


 representing an input pattern, each representing one of the input data groups, as the sequence of N numerical values.
7. A system as defined in claim 1, in which the input means includes data compilation means for combining N input variable values into one of the input data groups.
8. A system as defined in claim 1, in which: a) the intermediate layer storage means includes intermediate storage units for each of a plurality of intermediate storage layers; b) said plurality of intermediate storage layers includes the lowest intermediate layer and higher layers connected by the connection means between the lowest layer and the output layer storage means; and c) each intermediate memory unit has a corresponding intermediate weight vector and intermediate threshold value.
9. A system as defined in claim 8, in which the processor and control means is further provided for projecting the weight vectors in predetermined ones of the intermediate layers by augmenting these weight vectors with at least one projection value, and for normalizing these weight vectors.
10. A data processing method including the steps: a) representing each of a series of input data groups as a sequence of N numerical values to form a corresponding N-dimensional base input vector; b) augmenting each N-dimensional base input vector with j projection elements to form a projected input vector having N+j projected input elements, where j is a predetermined positive integer, and storing each projected input vector; 


 c) normalizing the elements of the projected input vector so that the magnitude of the projected input vector is equal to a predetermined input normalization value; d) for each of a plurality of intermediate nodes in a lowest intermediate layer: i) forming a weight vector having N+j weight elements; ii) constraining the magnitude of the weight vector to be equal to a predetermined weight normalization value; and iii) forming an intermediate nodal value as a weighted sum of the N+j input projection elements; and e) forming an output node value as a predetermined weight function of the intermediate nodal values, and storing the output node value in an output node.
11. A method as defined in claim 10, in which: the input vectors are 2-dimensional, representing vectors in a plane; the projected input vectors are 3-dimensional and represent vectors extending from the center of a sphere to the surface of the sphere; and the closed decision groups each correspond to closed regions on the surface of the sphere.
12. A method as defined in claim 10, in which j = 1.
13. A system as defined in claim 10, in which the weight normalization value is equal to the input normalization value.
14. A method as defined in claim 13, in which the weight normalization value and input normalization value are constants. 


 15. A method as defined in claim 10, further including the following steps: a) selecting a training set of known training vectors and a corresponding set of known goal vectors; b) generating an initial set of the N+j-dimensional weight vectors; c) for each intermediate node, selecting an initial intermediate threshold value; d) sequentially setting the base input vector equal to the training vectors; e) computing an error function value as a predetermined error function of the projected input training vectors, each of the weight vectors, and each of the threshold values; and f) adjusting the threshold values and the weight elements of each weight vector and repeating steps d) and e) until the error function value is less than a predetermined minimum error value.
16. A method as defined in claim 15, in which the threshold values and the weight vectors are adjusted as follows: for each set of threshold values and weight vectors for which the error function exceeds the minimum error value, optimizing the threshold values and weight vectors according to the following steps: i) recomputing the threshold values and weight vectors according to a predetermined minimization routine; ii) adjusting the recomputed weight vectors so that the magnitude of each weight vector is equal to the predetermined weight normalization value; and iii) sequentially reapplying the projected input training vectors as the projected input vectors.
17. A method as defined in claim 15, in which the initial weight vectors are set equal to predetermined N+j-dimensional prototype vectors, whereby each prototype 


vector corresponds to a respective one of the known training vectors.
18. A method as defined in claim 10, in which the input data groups consist of input patterns in a plurality of classes, further including the step of providing an output signal for each class corresponding to a probability that a current input pattern is in the corresponding class; whereby, increasing the complexity of the input vectors from dimension N to dimension at least N+j and normalizing both the weight vectors and the input vectors defines closed decision groups of possible output values using a single N+j-dimensional boundary region for each decision group.
19. A method as defined in claim 18, further including the following steps: a) separating the weight vectors and thresholds into pattern weight/threshold groups, with each pattern weight/threshold group corresponding to predetermined ones of the input pattern classes; and b) separately optimizing each pattern weight/ threshold group.
20. A method as defined in claim 18, in which each decision boundary is a hyperplane when the corresponding intermediate threshold value is set to a hyperplane value, and a hypersphere when the corresponding intermediate threshold value differs from the hyperplane value.
21. A method as defined in claim 10, in which the input data groups consist of sets of N input signals, corresponding to N input variables defining a K-dimensional output function, further including the step of providing at least K output nodal values for representing a current value of the output function; 


 whereby, increasing the complexity of the input vectors from dimension N to dimension at least N+j and normalizing both the weight vectors and the input vectors defines closed decision groups of possible output values using a single at least N+j dimensional boundary region for each decision group.
22. A method as defined in claim 10, further including the step of providing a plurality of intermediate layers, including the lowest intermediate layer and an uppermost intermediate layer, each having a plurality of intermediate nodes, each intermediate node having a corresponding intermediate nodal value, a corresponding intermediate weight vector and a corresponding intermediate threshold value.
23. A system as defined in claim 10, further including the step of transforming the intermediate nodal values using a transformation function, whereby each intermediate nodal value is represented as a smoothly interpolated transformed intermediate value constrained to lie between a finite maximum value and a finite minimum value.
24. A system as defined in claim 23, further including the step of computing an output weight vector for each output node, and for determining the value of each output node as a predetermined function of the weighted and biassed sums of the uppermost intermediate nodal values.
25. A method for identifying and classifying patterns, including the following steps: A) representing each of a series of input data groups as a sequence of N numerical values to form a corresponding N-dimensional base input vector, each input 


data groups consisting of input patterns in a plurality of classes;
B) augmenting each N-dimensional base input vector with j projection elements to form a projected input vector having N+j projected input elements, where j is a predetermined positive integer, and storing each projected input vector;
C) normalizing the elements of the projected input vector so that the magnitude of the projected input vector is equal to a predetermined normalization value;
D) for each of a plurality of intermediate nodes: i) forming a weight vector having N+j weight elements; ii) constraining the magnitude of the weight vector to be equal to the normalization value; and iii) forming an intermediate nodal value as a weighted sum of the N+j input projection elements; and
E) forming an output node value as a predetermined weight function of the intermediate nodal values, further including the step of providing an output signal for each class corresponding to a probability that a current input pattern is in the corresponding class; further including, in a training mode, the following steps: F) selecting a training set of known training vectors and a corresponding set of known goal vectors;
G) generating an initial set of the N+j-dimensional weight vectors;
H) for each intermediate node, selecting an initial intermediate threshold value;
I) sequentially setting the base input vector equal to the training vectors;
J) computing an error function value as a predetermined error function of the projected input training vectors, each of the weight vectors, and each of the threshold values; and 


 K) adjusting the intermediate threshold values and the weight elements of each weight vector as follows, and thereafter repeating steps I) and J) until the error function value is less than a predetermined minimum error value: for each set of intermediate threshold values and weight vectors for which the error function exceeds the minimum error value, optimizing the intermediate threshold values and weight vectors according to the following steps: i) recomputing the intermediate threshold values and weight vectors according to a predetermined minimization routine; ii) adjusting the recomputed weight vectors so that the magnitude of each weight vector is equal to the normalization value; and iii) sequentially reapplying the projected input training vectors as the projected input vectors; whereby, increasing the complexity of the input vectors from dimension N to dimension N+j and normalizing both the weight vectors and the input vectors defines closed decision groups of possible output values using a single N+j dimensional boundary region for each decision group.
26. A method as defined in claim 25, in which the initial weight vectors are set equal to predetermined N+j-dimensional prototype vectors, whereby each prototype vector corresponds to a respective one of the known training vectors.
27. A method as defined in claim 25, further including the following steps: a) separating the weight vectors and thresholds into pattern weight/threshold groups, with each pattern weight/ threshold group corresponding to one of the input pattern classes; and 


 b) separately optimizing each pattern weight/ threshold group.
28. A data processing method including the steps: a) representing each of a series of input data groups as a sequence of N numerical values to form a corresponding N-dimensional base input vector; b) augmenting each N-dimensional base input vector with j projection elements to form a projected input vector having N+j projected input elements, where j is a predetermined positive integer, and storing each projected input vector; c) normalizing the elements of the projected input vector so that the magnitude of the projected input vector is equal to a predetermined input normalization value; d) for each of a plurality of intermediate nodes in a lowest intermediate nodal layer, chosen among a plurality of sequential, intermediate nodal layers: i) forming a threshold value, and forming a weight vector having N+j weight elements; ii) constraining the magnitude of the weight vector to be equal to a predetermined corresponding weight normalization value; and iϋ) forming an intermediate nodal value as a weighted sum of the N+j input projection elements; e) for each of the plurality of intermediate nodes in each of the plurality of sequential, intermediate nodal layers other than the lowest intermediate nodal layer: i) forming a threshold value and forming a weight vector having at least p+j ' weight elements, where p is the number of nodes in the immediately preceding lower intermediate nodal layer and j ' is a predetermined positive integer; and ii) forming an intermediate nodal output value as a weighted sum of the intermediate nodal values of the preceding, lower nodal layer; 


 f) for each intermediate nodal layer pre-chosen as a projection layer, constraining the magnitude of the corresponding weight vectors to be equal to a predetermined corresponding weight normalization value; and g) for each ouput node in an output layer, forming an output weight vector and forming an output node value as a predetermined weight function of the intermediate nodal values, and storing the output node value in an output node.
29. A method as defined in claim 28, in which j = 1.
30. A system as defined in claim 28, in which the weight normalization value is equal to the input normalization value.
31. A method as defined in claim 30, in which the weight normalization value and input normalization value are constants.
32. A method as defined in claim 28, further including the following steps: a) selecting a training set of known training vectors and a corresponding set of known goal vectors; b) generating an initial set of weight vectors; c) for each intermediate node, selecting an initial intermediate threshold value; d) sequentially setting the base input vector equal to the training vectors; e) computing an error function value as a predetermined error function of the projected input training vectors, each of the weight vectors, and each of the threshold values; and f) adjusting the threshold values and the weight elements of each weight vector and repeating steps d) and 



e) until the error function value is less than a predetermined minimum error value.
33. A method as defined in claim 32, in which the threshold values and the weight vectors are adjusted as follows: for each set of threshold values and weight vectors for which set the error function exceeds the minimum error value, optimizing the threshold values and weight vectors according to the following steps: i) recomputing the threshold values and weight vectors according to a predetermined minimization routine; ii) adjusting the recomputed weight vectors so that the magnitude of each weight vector is equal to the predetermined weight normalization value; and iii) sequentially reapplying the projected input training vectors as the projected input vectors.
34. A method as defined in claim 32, in which the initial weight vectors are set equal to predetermined prototype vectors, whereby each prototype vector corresponds to a sampling based on the known training vectors.
35. A method as defined in claim 28, in which the input data groups consist of input patterns in a plurality of classes, further including the step of providing an output signal for each class corresponding to a probability that a current input pattern is in the corresponding class; whereby, increasing the complexity of the input vectors from dimension N to dimension at least N+j and normalizing both the weight vectors and the input vectors defines closed decision groups of possible output values using a single N+j-dimensional boundary region for each decision group. 


 36. A method as defined in claim 35, further including the following steps: a) separating the weight vectors and thresholds into pattern weight/threshold groups, with each pattern weight/ threshold group corresponding to one of the input pattern classes; and b) separately optimizing each pattern weight group.
37. A method as defined in claim 35, in which each decision boundary is a hyperplane when the corresponding intermediate threshold value is set to a hyperplane value, and a hypersphere when the .corresponding intermediate threshold value differs from the hyperplane value.
38. A method as defined in claim 28, in which, the input data groups consist of sets of N input signals, corresponding to N input variables defining a K-dimensional output function, further including the step of providing at least K output nodal values for representing a current value of the output function; whereby, increasing the complexity of the input vectors from dimension N to dimension at least N+j and normalizing both the weight vectors and the input vectors defines closed decision groups of possible output values using a single at least N+j dimensional boundary region for each decision group.
39. A system as defined in claim 28, further including the step of transforming the intermediate nodal values using a transformation function, whereby each intermediate nodal value is represented as a smoothly interpolated transformed intermediate value constrained to lie between a finite maximum value and a finite minimum value. 

</CLAIMS>
</TEXT>
</DOC>
