<DOC>
<DOCNO>EP-0612033</DOCNO> 
<TEXT>
<INVENTION-TITLE>
learning processing system
</INVENTION-TITLE>
<CLASSIFICATIONS>G06N308	G06N300	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06N	G06N	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06N3	G06N3	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
The present invention is concerned with a learning processing system adapted to cause the 
signal processing section (50) formed by a neural network (L
I
, L
H
, L
O
) to undergo signal 
processing pursuant to the back-propagation learning rule, wherein the local minimum state in the 

course of the learning processing may be avoided by learning the coefficient of coupling strength 
while simultaneously increasing the number of the unit (u
H1
 to u
Hy
) of the intermediate layer 
(L
H
). 

</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
SONY CORP
</APPLICANT-NAME>
<APPLICANT-NAME>
SONY CORPORATION
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
HIRAIWA ATSUNOBU
</INVENTOR-NAME>
<INVENTOR-NAME>
HIRAIWA, ATSUNOBU
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
This invention relates to a signal processing apparatus or system carrying out
signal processing with the use of a so-called neural network made up of a plurality of
units each taking charge of signal processing corresponding to that of a neuron, and a
learning processing apparatus or system causing a signal processing section by said
neural network to undergo a learning processing in accordance with the learning rule
of back propagation.The learning rule of back propagation, which is a learning algorithm of the
neural network, has been tentatively applied to signal processing, including high speed
image processing or pattern recognition, as disclosed in "Parallel Distributed
Processing", vol. 1, The MIT Press, 1986 or "Nikkei Electronics", issue of
August 10, 1987, N° 427, pp 115 to 124. The learning rule of back propagation is also
applied, as shown in Fig. 1, to a multistorey neural network having an intermediate
layer 2 between an input layer 1 and an output layer 3.Each unit uj of the neural network shown in Fig. 1 issues an output value
which is the total sum netj of output values Oi of a unit uj coupled to the unit uj by a
coupling coefficient Wji, transformed by a predetermined function f, such as a
sigmoid function. That is, when the value of a pattern p is supplied as an input value to
each unit uj of the input layer 1, an output value Opj of each unit uj of the intermediate
layer 2 and the output layer 3 is expressed by the following formula (1)
Opj = fj(netpj)= fj(Σi Wji•Opji)The output value Opj of the unit uj of the output layer 3 may be obtained by
sequentially computing the output values of the inputs uj, each corresponding to a
neuron, from the input layer 1 towards the output layer 3.In accordance with the back-propagation learning algorithm, the processing of
learning consisting in modifying the coupling coefficient Wji so as to minimize the
total sum Ep of square errors between the actual output value Opj of each unit uj of the 
output layer 3 on application of the pattern p and the desirable output value tpj, that is
the teacher signal,

is sequentially performed from the output layer 3 towards the input layer 1. By such
processing of learning, the output value Opj closest to the value tpj of the teacher
signal is output from the unit uj of the output layer 3.If the variant Δ Wji of the coupling coefficient Wji which minimizes the total
sum Ep of the square errors is set so that
Δ Wji α - ∂ Ep/∂Wji
the formula (3) may be rewritten to
Δ Wji = η . δpj Opj
as explained in detail in the
</DESCRIPTION>
<CLAIMS>
A learning processing system (50, 60) comprising:

a signal processing section (50) composed of a multi-layer neural network
having an input layer (L
I
), a bidden layer (L
H
) and an output layer (L
O
), the layers being
made up of units u
I1
 to u
Ix
, u
H1
 to u
Hy
 and u
O1
 to u
Oz
, respectively, each unit
corresponding to a neuron; and
a learning processing section (60) executing a learning process using a back-propagation
learning algorithm, the process consisting in sequentially modifying, from

the output layer towards the input layer, the coupling coefficients W
ji
 of all units j in the
hidden and in the output layer by a variant ΔW
ji
 so as to minimize the total sum of
square errors between the actual output O
pj
 of unit j in the output layer (L
O
) produced
from an input signal pattern (p) and the desirable output value t
pj
 (teacher signal) for said
unit j in the output layer (L
O
), whereby W
ji
 is the weight for the signal from the ith to
the jth unit,
the learning processing section being fed with a desired output value t
pj
 as a teacher signal
for the output value O
pj
 of the unit j in the output layer (L
O
) for the input patterns p
entered into the input layer (L
I
),
the learning processing section (60) computing the error value for each unit in the
output layer
 and in the bidden layer,
said learning process being executed repeatedly until the total sum (E) of the
square error between the desired output afforded as the teacher signal and the output

signal becomes sufficiently small;

   characterised in that the learning processing section (60) comprises control means
for increasing the number of units in the bidden layer (L
H
), during the repeated
execution of said learning process, either periodically or when a local minimum state of

the signal processing system has been detected, said learning processing section (GO)
being adapted in subsequent repeated executions of the learning process to perform

learning processing of the coefficients W
ji
 of coupling strength in respect of the
increased number of units in the hidden layer. 
The learning processing system of claim 1, wherein the control means of the
learning processing section (60) is adapted to detect a local minimum state by comparing

successive values of a first variable, Lms, where

</CLAIMS>
</TEXT>
</DOC>
