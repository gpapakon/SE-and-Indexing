<DOC>
<DOCNO>EP-0649542</DOCNO> 
<TEXT>
<INVENTION-TITLE>
METHOD AND APPARATUS FOR A UNIFIED PARALLEL PROCESSING ARCHITECTURE.
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F110	G06F110	G06F938	G06F938	G06F1206	G06F1206	G06F1316	G06F1316	G06F1516	G06F15173	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G06F	G06F	G06F	G06F	G06F	G06F	G06F	G06F	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F1	G06F1	G06F9	G06F9	G06F12	G06F12	G06F13	G06F13	G06F15	G06F15	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A unified parallel processing architecture connects together an extendible number of clusters (100a, 100b, 100c, 100d) of multiple numbers of processors (102a, 102b, 102c, 102d) to create a high performance parallel processing computer system. Multiple processors are grouped together into four or more physically separable clusters (100a, 100b, 100c, 100d) each cluster having a common cluster shared memory (104a, 104b, 104c, 104d) that is symmetrically accessible by all of the processors in that cluster; however, only some of the clusters are adjacently interconnected. Clusters are adjacently interconnected (100a, 100c) to form a floating shared memory (104a, 104c) if certain memory access conditions relating to relative memory latency and relative data locality can create an effective shared memory parallel programming environment. A shared memory model can be used with programs that can be executed in the cluster shared memory of a single cluster, or in a floating shared memory that is defined across an extended shared memory space comprised of the cluster shared memories of any set of adjacently interconnected clusters. The adjacent interconnections of multiple clusters of processors to create a floating shared memory effectively combines all three types of memory models, pure shared memory (20, 22, 24), extended shared memory (30, 34, 32) and distributed shared memory (14, 12, 10) into a unified parallel processing architecture.
</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
CRAY RESEARCH INC
</APPLICANT-NAME>
<APPLICANT-NAME>
CRAY RESEARCH, INC.
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
BEARD DOUGLAS R
</INVENTOR-NAME>
<INVENTOR-NAME>
CHEN STEVE S
</INVENTOR-NAME>
<INVENTOR-NAME>
PRIEST EDWARD C
</INVENTOR-NAME>
<INVENTOR-NAME>
SPIX GEORGE A
</INVENTOR-NAME>
<INVENTOR-NAME>
VANDYKE JAMES M
</INVENTOR-NAME>
<INVENTOR-NAME>
WASTLICK JOHN M
</INVENTOR-NAME>
<INVENTOR-NAME>
BEARD, DOUGLAS, R.
</INVENTOR-NAME>
<INVENTOR-NAME>
CHEN, STEVE, S.
</INVENTOR-NAME>
<INVENTOR-NAME>
PRIEST, EDWARD, C.
</INVENTOR-NAME>
<INVENTOR-NAME>
SPIX, GEORGE, A.
</INVENTOR-NAME>
<INVENTOR-NAME>
VANDYKE, JAMES, M.
</INVENTOR-NAME>
<INVENTOR-NAME>
WASTLICK, JOHN, M.
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
 METHOD AND APPARATUS FOR AUNIFIED PARALLEL PROCESSING ARCHITECTURETECHNICAL FIELDThe present invention relates generally to parallel processing computer systems for performing multiple-instruction-multiple-data (MIMD) parallel processing. More particularly, the present invention relates to a method and apparatus for a unified parallel processing architecture for high performance MIMD multiprocessors that organizes the multiprocessors into four or more physically separable clusters, only some of which are adjacently interconnected, and provides for a shared memory model to be used with programs executed in the floating shared memory space that is defined across any adjacently interconnected clusters, and a distributed memory model to be used with any programs executed across non-adjacently interconnected clusters.PRIOR ART The field of parallel computing has received increasing attention in recent years as computer designers seek to increase the effective processing capabilities of computer processing systems. Most of the current work relating to high performance parallel processing has focused on multiple- instruction-multiple-data (MIMD) parallel processing systems. Current architectures for MIMD parallel processing systems use one of two different memory models for sharing data within a program that is to be 

executed in parallel: distributed memory vs. shared memory. In a distributed memory model, data is stored in the private local memory of each processor and is communicated among processors by some type of message passing scheme. In a shared memory model, data is stored in a common shared memory that is equally accessible to all processors in the parallel processing system. An excellent summary of current MIMD parallel architectures is set forth in Almasi and Gottlieb, Highly Parallel Computing. (1989) Chpt. 10, pgs. 354-475.From the programmer's perspective, the most attractive parallel processing system is one that uses a shared memory model having a globally shared physical address space. A shared address space leaves the choice of the parallel programming model up to the programmer and provides a more flexible problem solution space. In addition, most present software programs are written for a shared memory model, as opposed to a distributed or private memory model. To effectively port an existing software program from a shared memory model to a private memory model can require significant reprogramming effort. One of the primary reasons why it is easier to program a shared memory model is
</DESCRIPTION>
<CLAIMS>
CLAIMS
1. A unified parallel processing architecture for connecting together an extendible number of clusters of multiple numbers of processors to create a parallel processing computer system, the unified parallel processing architecture comprising: four or more clusters of multiprocessors, each cluster comprising: a cluster shared memory means for storing and retrieving data and instructions for one or more parallel tasks of one or more computer programs; two or more processor means for executing the instructions and operating directly on the data for the parallel tasks that are stored in the cluster shared memory means; one or more input /output port means for transferring the data and instructions for the parallel tasks between the cluster shared memory means and one or more external data sources via one or more block-by-block communication paths; and connection node means for connecting the cluster shared memory means with all of the processor means in the cluster such that access to the clustered shared memory means by all of the processor means in the cluster is symmetrical; means for providing a plurality of floating shared memories by adjacently connecting some, but not all of the clusters, via one or more word-by-word communication paths to create a floating shared memory that is defined by the cluster shared memory means of the cluster and the cluster shared memory means of any adjacently interconnected clusters and that can be programmed using a shared memory model; and means for providing an extended distributed memory among 


 any non-adjacently interconnected clusters that defines an extendible address space encompassing the cluster shared memory means of all clusters that can be programmed using a distributed memory model.
2. The parallel processing architecture of claim 1 further comprising: means for providing an extendible clock signal to each cluster.
3. The parallel processing architecture of claim 1 further comprising: means for providing an extendible control mechanism for each cluster that is accessible by a distributed operating system program executing on any of the processor means in any of the clusters for distributively controlling the communication and coordination among a plurality of processes executing in parallel in the computer processing system.
4. The parallel processing architecture of claim 1 wherein the means for providing a plurality of floating shared memories comprises: two or more cluster connection means for each cluster for adjacently connecting all of the processor means in the cluster with the cluster shared memory means in two or more, but not all, of the other clusters via one or more word-by-word communication paths such that the processor means can execute the instructions and operate directly on the data for the parallel tasks that are stored in any of the cluster shared memory means of any of the adjacently connected clusters.
5. The parallel processing architecture of claim 4 wherein either an input /output port means or a cluster connection means can be interchangeably connected to shared memory means via the connection node means and wherein the total number of input /output port means 


and cluster connection means is fixed for each cluster and the ratio of input/output is configurable.
6. The parallel processing architecture of claim 1 wherein the means for providing an extended distributed memory comprises: a message passing scheme implemented in an operating system program executing in one or more processors of each cluster such that messages may be passed among the processor means in all of the clusters in the parallel processing computer system.
7. The parallel processing architecture of claim 1 wherein the means for providing an extended distributed memory comprises: a floating scheme implemented in an operating system program executing in one or more of the processor means in each cluster such that one or more segments of data or instructions for a parallel task are moved from the floating shared memory of one set of adjacently interconnected clusters to the floating shared memory of another set of adjacently interconnected clusters.
8. The parallel processing architecture of claim 1 where the connection node means comprises: two or more arbitration node means, each arbitration node means having two or more unique unidirectional direct connection paths between the cluster shared memory means and a unique two or more of the processor means for symmetrically multiplexing the unique two or more processor means to the clustered shared memory means.
9. The parallel processing architecture of claim 1 wherein clusters are adjacently interconnected if the average memory latency of an inter- cluster memory access by a processor means in one cluster to the cluster shared memory means of another cluster is less than ten times greater 


than the average memory latency of an intra-cluster memory access by a processor means to the cluster shared memory means within that cluster and the average number of the inter-cluster memory accesses is less than ten percent of the number of intra-cluster memory accesses for an average parallel task.
10. The parallel processing architecture of claim 9 wherein clusters are adjacently interconnected if the average memory latency of an inter- cluster memory access is less than five times greater than the average memory latency of an intra-cluster memory access and the average number of the inter-cluster memory accesses is less than five percent of the number of intra-cluster memory accesses for an average parallel task.
11. The parallel processing architecture of claim 9 wherein clusters are adjacently interconnected if the average memory latency of an inter- cluster access is less than about two times greater than the average memory latency of an intra-cluster access and the average number of the inter-cluster memory accesses is less than one percent of the number of intra-cluster memory accesses for an average parallel task.
12. A method for providing a unified parallel processing architecture for a parallel processing computer system that can process one or more computer programs in parallel, each computer program having a problem space of a predetermined size, the method comprising the steps of: (a) providing 4 or more clusters of multiprocessors, each cluster having 2 or more processors that are symmetrically connected to a cluster shared memory such that each processor in the cluster has a similar average memory latency when accessing the cluster shared memory; (b) adjacently interconnecting each cluster to some, but not all, of the other clusters; (c) remotely interconnecting each cluster to all other clusters to 


 which that cluster is not adjacently interconnected; (d) using a shared memory model to process in parallel any computer programs having a problem space of a size that is less than the size of a floating shared memory defined by the collective cluster shared memories of a set of adjacently interconnected clusters; and (e) using a distributed memory model to process in parallel any computer programs having a problem space of a size that is greater than the size of the floating shared memory.
13. The method of claim 12 wherein clusters are adjacently interconnected if the average memory latency of an inter-cluster memory access by a processor in one cluster to the cluster shared memory of another cluster is less ten times greater than the average memory latency of an intra-cluster memory access by a processor to the cluster shared memory within that cluster and the average number of the inter-cluster memory accesses is less than ten percent of the number of intra-cluster memory accesses for an average parallel task.
14. The method of claim 13 wherein clusters are adjacently interconnected if the average memory latency of an inter-cluster memory access is less than five times greater than the average memory latency of an intra-cluster memory access and the average number of the inter- cluster memory accesses is less than five percent of the number of intra- cluster memory accesses for an average parallel task.
15. The method of claim 13 wherein clusters are adjacently interconnected if the average memory latency of an inter-cluster access is less than about two times greater than the average memory latency of an intra-cluster access and the average number of the inter-cluster memory accesses is less than one percent of the number of intra-cluster memory accesses for an average parallel task. 


16. The method of claim 12 further comprising the step of: (f) providing each cluster with an extendible clock signal.
17. The method of claim 12 further comprising the step of: (f) providing each cluster with an extendible control mechanism that is accessible by a distributed operating system program executing on any of the processors in any of the clusters for distributively controlling the communication and coordination among a plurality of processes executing in parallel in the computer processing system.
18. The method of claim 12 wherein step (e) comprises: using a message passing scheme implemented in an operating system program executing in one or more processors of each cluster to pass messages among the processors in all of the clusters in the parallel processing computer system.
19. The method of claim 12 wherein step (e) comprises: using a floating scheme implemented in an operating system program executing in one or more of the processor means in each cluster to move one or more segments of data or instructions for a parallel task from the floating shared memory of one set of adjacently interconnected clusters to the floating shared memory of another set of adjacently interconnected clusters.
20. A unified parallel processing architecture for connecting multiple processors to multiple memory components to form a parallel processing computer system comprising: four or more cluster means for connecting a plurality of processors to one or more memory components such that the processors can symmetrically access the one or more memory 


 components using a shared memory model; two or more floating shared memory means for uniquely and adjacently connecting some, but not all, of the plurality of the cluster means such that the processors in a floating shared memory means can access any of the memory components in the floating shared memory means using an extended shared memory model; and distributed memory communication means for providing communication among any of the cluster means that are not adjacently connected by a floating shared memory means such that any of the processors in the parallel processing computer system can access any of the memory components in the parallel processing computer system using a distributed memory model.
21. A multiprocessor cluster adapted to be connected to other similar clusters in a parallel processing computer system, the multiprocessor cluster comprising: a cluster shared memory means for storing and retrieving data and instructions for one or more parallel tasks of one or more computer programs; two or more processor means for executing the instructions and operating directly on the data for the parallel tasks that are stored in the cluster shared memory means; one or more input /output port means for transferring the data and instructions for the parallel tasks between the cluster shared memory means and one or more external data sources via one or more block-by-block communication paths; connection node means for connecting the cluster shared memory means with all of the processor means in the cluster such that access to the clustered shared memory means by all of the processor means in the cluster is symmetrical; and two or more cluster connection means for adjacently 


 connecting all of the processor means in the cluster with the cluster shared memory means of two or more other clusters via one or more word-by-word communication paths such that the processor means can execute the instructions and operate directly on the data for the parallel tasks that are stored in any of the cluster shared memory means of any of the adjacently connected clusters, wherein the input /output port means and the cluster connection means can be interchangeably connected to the shared memory means via the connection node means and wherein the total number of input /output port means and cluster connection means is fixed for each cluster and the ratio of input /output is configurable.
22. The multiprocessor cluster of claim 21 wherein the connection node means comprises: two or more arbitration node means, each arbitration node means having two or more unique unidirectional direct connection paths between the cluster shared memory means and a unique two or more of the processor means for symmetrically multiplexing the unique two or more processor means to the clustered shared memory means.
23. The multiprocessor cluster of claim 22 wherein each arbitration node means also provides a unique connection to the shared memory means for two input/ output port means or one cluster connection means.
24. The multiprocessor cluster of claim 23 wherein the cluster communication means comprises: an inter-cluster dispatch means operably connected to all of the arbitration node means for receiving one or more inter-cluster memory accesses from any of the processors in the multiprocessor cluster; 


 a cluster connection output port means operably connected to one of the arbitration node means for transferring one or more inter-cluster memory accesses from the inter-cluster dispatch means to any adjacently connected cluster; and a cluster connection input port means operably connected to the same one of the arbitration node means for receiving one or more inter-cluster memory accesses from any of the processors in any adjacently connected cluster and presenting the one or more inter-cluster memory accesses from any of the processors in any adjacently connected cluster to the cluster shared memory of the multiprocessor cluster via the same one
'
of the arbitration node means.
25. An extendible clock mechanism for providing a frequency locked, phase-independent clock signal to a plurality of physically separable computer processor system components comprising: a common clock oscillator for generating the frequency locked clock signal; and in each of the computer processor system components: clock means for receiving the frequency locked clock signal and producing a frequency locked, phase-independent local clock signal; transmitter means operably connected to the clock means for transmitting an inter-component data signal and clock signal that are clocked by the local clock signal; and receiver means operably connected to the clock means and the transmitter means of an other component for receiving the inter-component data signal and clock signal from the transmitter means in the other component, the receiver means including: a receiver latch means for latching the data signal from the transmitter means that is clocked by 


 the clock signal recieved from the transmitter means; a buffer means operably connected to the receiver latch means for storing a plurality of latched data signals; a write pointer means that is also clocked by the clock signal received from the transmitter means for determining where the data signal in the receiver latch means will be written into the buffer means; buffer output means for providing a selected latched data signal from the buffer means to the component; and a read pointer means that is clocked by the local clock signal for determining where in the buffer means the selected latched data signal will be read.
26. The extendible clock mechanism of claim 25 wherein the buffer means is of size 2N+1, where N is the transmission uncertainty in an inter-component communication path over which the clock signal and data signal are transmitted, and wherein the write pointer means and the read pointer means are spaced apart by at least N storage locations in the buffer means. 

</CLAIMS>
</TEXT>
</DOC>
