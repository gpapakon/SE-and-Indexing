<DOC>
<DOCNO>EP-0621556</DOCNO> 
<TEXT>
<INVENTION-TITLE>
A process for combining the results of several classifiers.
</INVENTION-TITLE>
<CLASSIFICATIONS>G06K903	G06K903	G06K962	G06K962	G06K968	G06K968	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06K	G06K	G06K	G06K	G06K	G06K	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06K9	G06K9	G06K9	G06K9	G06K9	G06K9	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
A process for successfully combining the 
results of several classifiers provides a method for 

calculating confidences of each classification 
decision for every classifier involved. Confidences 

are then combined according to the Dempster-Shafer 
Theory of Evidence. Initially, basic probability 

assignments for each of the classifiers are 
calculated and used to calculate confidences for 

each classifier. The confidences for all of the 
classifiers are then combined. The combined 

confidences are then used to determine a class for 
the data input to the classifiers. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
EASTMAN KODAK CO
</APPLICANT-NAME>
<APPLICANT-NAME>
EASTMAN KODAK CO
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
ROGOVA GALINA C O EASTMAN KODA
</INVENTOR-NAME>
<INVENTOR-NAME>
ROGOVA GALINA C O EASTMAN KODA
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The present invention is directed to the 
field of pattern classification and recognition and, 
more particularly, to a mechanism for calculating a 
confidence of any classification decision. Pattern recognition problems, such as 
classification of machine or hand-printed character, 
are currently solved with some accuracy by using 
traditional classifiers or neural networks of 
different types. It is easier in many cases to 
apply several classifiers to the same recognition 
task to improve recognition performance in a 
combined system, instead of inventing a new 
architecture or a feature extractor to achieve the 
same accuracy. However, it is necessary to assign a 
measure of evidence to a classification decision of 
each classifier in a system, to be able to combine 
them. Unfortunately, such assignments demand 
numerous approximations, especially when the number 
of classes is large. This creates computational 
difficulties, and can decrease the quality of the 
recognition performance. It is known in the art to use the 
Dempster-Shafer Theory of Evidence as a tool for 
representing and combining measures of evidence. In 
the existing art, U.S. Patent No. 5,123,057 uses the 
Dempster-Shafer Theory of Evidence to calculate a 
degree of match between data events portions and 
model parts. Evidence is collected and processed 
after preliminary matching has been performed using 
the Dempster-Shafer Theory of Evidence.  Additionally, U.S. Patent No. 5,077,807 to Bokser 
describes a method for processing input feature 
vectors. Again, the '807 patent relates to a 
preprocessing means for pattern recognition. So 
although the prior art addresses the problem of 
pattern classification and recognition, the existing 
art does not address the use of the Dempster-Shafer 
Theory of Evidence as a postrecognition or 
postprocessing tool to combine results of several 
classifiers. It is seen then that it would be 
desirable to have a means for improving the results 
of classification. The present invention improves the 
results of classification by successfully combining 
the results of several classifiers. The invention 
also improves the results of classification by 
eliminating numerous approximations which result in 
a decreased accuracy and quality of the 
classification and recognition performance. Specifically, the present invention uses a distance 
measure between a classifier output vector and a 
mean vector for a subset of training data 
corresponding to each class. Using these distances 
for basic
</DESCRIPTION>
<CLAIMS>
A method for combining results of several classifiers 
comprises the steps of: 

   calculating basic probability assignments for 
each of the classifiers; 

   using the basic probability assignments to 
calculate confidences for each classifier; 

   combining the confidences for all of the 
classifiers; and 

   using the combined confidences to determine a 
class for data input to the classifiers. 
A method as claimed in claim 1 wherein the steps of 
calculating basic probability assignments and using the 

basic probability assignments to calculate confidences for 
each classifier comprise the step of applying a suitable 

theory of evidence. 
A method as claimed in claim 1 wherein the step of 
using the basic probability assignments to calculate 

confidences for each classifier comprises the steps of: 
   using a distance measure between a classifier 

output vector and a mean vector for a subset of training 
data corresponding to each class; and 

   calculating evidences for all classification 
decisions for each classifier, using the distances as basic 

probability assignments. 
A method as claimed in claim 3 wherein the distance 
measure comprises one of two almost equivalent distance 

measures. 
A method as claimed in claim 4 wherein the first 
distance measure comprises a cosine between a classifier 

output vector and a mean vector for a subset of training 

data corresponding to each class. 
A system for combining the results of several 
classifiers comprising: 

   means for calculating basic probability 
assignments for each of the classifiers; 

   means for calculating confidences for each 
classifier from the basic probability assignments; 

   means for combining the confidences for all of 
the classifiers; and 

   means for determining a class for data input to 
the classifiers from the combined confidences. 
A system as claimed in claim 6 wherein the means for 
calculating basic probability assignments and confidences 

for each classifier comprise a suitable theory of evidence. 
A system as claimed in claim 7 wherein the means for 
calculating confidences for each classifier comprises: 

   a distance measure between a classifier output 
vector and a mean vector for a subset of training data 

corresponding to each class; and 
   means for calculating evidences for all 

classification decisions for each classifier, using the 
distances as basic probability assignments. 
A system as claimed in claim 8 wherein the distance 
measure comprises one of two almost equivalent distance 

measures. 
A system as claimed in claim 9 wherein the second 
distance measure comprises a function of Euclidean distance 

between a classifier output vector and a mean vector for a 
subset of training data corresponding to each class. 
</CLAIMS>
</TEXT>
</DOC>
