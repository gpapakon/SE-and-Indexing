<DOC>
<DOCNO>EP-0652525</DOCNO> 
<TEXT>
<INVENTION-TITLE>
High efficiency learning network
</INVENTION-TITLE>
<CLASSIFICATIONS>G06N300	G06N3063	G06G760	G06N304	G06F1518	G06F1518	G06G700	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06N	G06N	G06G	G06N	G06F	G06F	G06G	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06N3	G06N3	G06G7	G06N3	G06F15	G06F15	G06G7	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
In a learning network (or neural network), nodal outputs are discretized 
to values of S2
n
 where n is an integer and S is equal to +1 or -1. During forward 
propagation, this offers the advantage of forming a product of a nodal output and a 

weight using a simple shift operation rather than a multiply operation. Replacing 
multiply operations with shift operations through out a neural network improves 

response times and permits building larger networks that have broader applicability. 
Training is also improved by increasing the efficiency of backward propagation. The 

multiplications involved in backward propagation are reduced to shift operations by 
discretizing the errors associated with each node so that they are represented as S2
n
 
where n is an integer and S is equal to +1 or -1. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
AT 
&
 T CORP
</APPLICANT-NAME>
<APPLICANT-NAME>
AT
&
T CORP.
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
SIMARD PATRICE YVON
</INVENTOR-NAME>
<INVENTOR-NAME>
SIMARD, PATRICE YVON
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The invention relates to learning networks
with high efficiency forward propagation and backward propagation, more specifically
to a computational network.Learning networks or neural networks are available in a variety of
architectures that are implemented in hardware, software or a combination of
hardware and software. U. S. patent 5,067,164 entitled "Hierarchical Constrained
Automatic Learning Neural Network for Character Recognition" and U. S. patent
5,058,179 entitled "Hierarchical Constrained Automatic Learning Network for
Character Recognition" disclose two of many architectures available to learning
networks. Learning networks comprise computational nodes or neurons that perform
an activation function using a sum of its inputs as a operand to provide a output.
These nodes are typically arranged in layers with the output of a node from one layer
being connected to the input of several nodes of the next layer. Each input to a node
is typically multiplied by a weight before being summed by the node.Learning networks or neural networks typically include a large number
of nodes or neurons that receive inputs from may other neurons within the learning
network. This results in an architecture where there are many interconnections
between the outputs and inputs of the nodes. As mentioned earlier, these
interconnections normally include a multiplication function. As a result, large neural
networks require a large number of multiplications to produce an output. In
addition, these multiplications typically involve multiplying a 16-bit word by
another 16-bit word and thereby require a great deal of computational power. As a
result of these computational requirements, large neural networks often have slow
response times and slow training times.With the ever-expanding demand for products that involve functions
such as speech recognition, handwriting recognition and pattern recognition, there is
an increasing need for large neural networks with fast response times and short
training times. H.K. Kwan and C.Z. Tang, in Proceedings of the 36th Midwest
Symposium on Circuits and Systems, 16 August 1993, Detroit USA, pages 1085-1088,
'Multiplierless Multilayer Feedforward Neural Networks', disclose a feedforward neural
network in which the weights in layers subsequent to the first are restricted to powers of
two. Such a network is also disclosed by M. Marchesi et al. Proceedings of the 1990
IEEE International Symposium on Circuits and Systems, 1-3 May 1990, New Orleans
USA, vol. 4, pages 2951-2954, 'Design of
</DESCRIPTION>
<CLAIMS>
A computational network that produces a plurality of network outputs
in response to a plurality of network inputs, comprising:


a plurality of first layer computational nodes (18, 20, 22, 24) forming a
first layer of a computational network, each of said first layer computational nodes

receiving at least one input resulting from at least one of a plurality of computational
network inputs to produce a first layer output, and;
a plurality of second layer computational nodes (26, 28, 30, 32) forming
a second layer of the computational network, each of said plurality of second layer

computational nodes receiving a plurality of inputs to produce a second layer output,
each of said plurality of inputs being a product of a weight value (W
j,i
) and a said first
layer output produced by one of said plurality of first layer computational nodes, said

second layer output being
produced by using a sum of said plurality of inputs

as an operand of a first activation function, said second layer output being 
used to produce at least one of a plurality of network outputs;

CHARACTERIZED IN THAT
each said first layer output is discretized to integer powers of 2; and
in that

product means are provided for producing said product by shifting said weight value as many
times as indicated by said corresponding power of 2.
The computational network of claim 1, characterized in that said first
layer output is in the form of S2
n
 where n is a nonzero integer and S is Â± 1.
The computational network of claim 2, further characterized by
product means for producing said product by summing n and an exponent portion of

said weight value.
The computational network of any of the preceding claims, further
characterized in that said plurality of network inputs are discretized to a power of 2.
The computational network of any of the preceding claims, further
characterized in that each of said first layer computational nodes produces said first

layer output by using a sum of a plurality of first layer inputs as an operand of a
second activation function.
</CLAIMS>
</TEXT>
</DOC>
