<DOC>
<DOCNO>EP-0621531</DOCNO> 
<TEXT>
<INVENTION-TITLE>
Interactive computer system recognizing spoken commands
</INVENTION-TITLE>
<CLASSIFICATIONS>G06F1500	G06F1500	G10L1500	G06F316	G10L1522	G10L1520	G10L1500	G06F316	G10L1526	</CLASSIFICATIONS>
<CLASSIFICATIONS-THIRD>G06F	G06F	G10L	G06F	G10L	G10L	G10L	G06F	G10L	</CLASSIFICATIONS-THIRD>
<CLASSIFICATIONS-FOURTH>G06F15	G06F15	G10L15	G06F3	G10L15	G10L15	G10L15	G06F3	G10L15	</CLASSIFICATIONS-FOURTH>
<ABSTRACT>
An interactive computer system having a processor executing 
a target computer program, and having a speech recognizer 

for converting an utterance into a command signal for the 
target computer program. The target computer program has a 

series of active program states occurring over a series of 
time periods. At least a first active-state image is displayed 

for a first active state occurring during a first 
time period. At least one object displayed in the first 

active-state image is identified, and a list of one or more 

first active-state commands identifying functions which can 
be performed in the first active state of the target computer 

program is generated from the identified object. A 
first active-state vocabulary of acoustic command models 

for the first active state comprises the acoustic command 
models from a system vocabulary representing the first 

active-state commands. A speech recognizer measures the 
value of at least one feature of an utterance during each 

of a series of successive time intervals within the first 
time period to produce a series of feature signals. The 

measured feature signals are compared to each of the 
acoustic command models in the first active-state vocabulary 

to generate a match score for the utterance and each 
acoustic command model. The speech recognizer outputs a 

command signal corresponding to the command model from the 
first active-state vocabulary having the best match score. 


</ABSTRACT>
<APPLICANTS>
<APPLICANT-NAME>
IBM
</APPLICANT-NAME>
<APPLICANT-NAME>
INTERNATIONAL BUSINESS MACHINES CORPORATION
</APPLICANT-NAME>
</APPLICANTS>
<INVENTORS>
<INVENTOR-NAME>
ANDRESHAK JOSEPH CHARLES
</INVENTOR-NAME>
<INVENTOR-NAME>
DAGGETT GREGG H
</INVENTOR-NAME>
<INVENTOR-NAME>
KARAT JOHN
</INVENTOR-NAME>
<INVENTOR-NAME>
LEVY STEPHEN ERIC
</INVENTOR-NAME>
<INVENTOR-NAME>
LUCASSEN JOHN
</INVENTOR-NAME>
<INVENTOR-NAME>
MACK ROBERT LAWRENCE
</INVENTOR-NAME>
<INVENTOR-NAME>
ANDRESHAK, JOSEPH CHARLES
</INVENTOR-NAME>
<INVENTOR-NAME>
DAGGETT, GREGG H.
</INVENTOR-NAME>
<INVENTOR-NAME>
KARAT, JOHN
</INVENTOR-NAME>
<INVENTOR-NAME>
LEVY, STEPHEN ERIC
</INVENTOR-NAME>
<INVENTOR-NAME>
LUCASSEN, JOHN
</INVENTOR-NAME>
<INVENTOR-NAME>
MACK, ROBERT LAWRENCE
</INVENTOR-NAME>
</INVENTORS>
<DESCRIPTION>
The invention relates to interactive computer systems in
which a user provides commands to a target computer program
executing on the computer system by way of an input device.
The input device may be, for example, a keyboard, a mouse
device, or a speech recognizer. For each input device, an
input signal generated by the input device is translated
into a form usable by the target computer program.An interactive computer system in which the user can
provide commands by speaking the commands may consist of a
processor executing a target computer program having commands
identifying functions which can be performed by the
target computer program. The computer system further
includes a speech recognizer for recognizing the spoken
commands and for outputting command signals corresponding
to the recognized commands. The speech recognizer recognizes
a spoken command by measuring the value of at least
one feature of an utterance during each of a series of
successive time intervals to produce a series of feature
signals, comparing the measured featured signals to each of
a plurality of acoustic command models to generate a match
score for the utterance and each acoustic command model,
and outputting a command signal corresponding to the
command model having the best match score.The set of utterance models and words represented by the
utterance models which the speech recognizer can recognize
is referred to as the system vocabulary. The system vocabulary
is finite and may, for example, range from one utterance
model to thousands of utterance models. Each
utterance model may represent one word, or may represent a 
combination of two or more words spoken continuously
(without a pause between the words).The system vocabulary may contain, for example, utterance
models of all of the commands to which the target computer
program is capable of responding. However, as the number
of utterance models increases, the time required to perform
utterance recognition using the entire system vocabulary
increases, and the recognition accuracy decreases.Generally, a target computer program has a series of active
states occurring over a series of time periods. For each
active state, there may be a list of active state commands
identifying functions which can be performed in the active
state. The active state commands may be a small subset of
the system vocabulary. The translation of an uttered
command to a form usable by the target computer program in
one state of the target computer program may be different
from the translation of
</DESCRIPTION>
<CLAIMS>
An interactive computer system comprising:

a processor (10) executing a target computer program having
a series of active program states occurring over a

succession of time periods, said target computer
program generating active state image data signals

representing an active state image for an active state
of the target computer program occurring during each

time period, each active state image containing one or
more objects;
means (12) for displaying at least a first active-state
image for a first active state occurring during a first

time period;
means (26) for identifying an object displayed in the first
active-state image, and for generating from an

identified object displayed in the first active-state
image a list of one or more first active-state commands

identifying a first active-state function which can be
performed in the first active state of the target

computer program;
means (28) for storing a system vocabulary of acoustic
command models, each acoustic command model

representing one or more series of acoustic feature
values representing an utterance of one or more words

associated with the acoustic command model;
means (30) for identifying a first active-state vocabulary
of acoustic command models for the first active state,

the first active-state vocabulary comprising the
acoustic command models from the system vocabulary

representing the first active-state commands, wherein
the first active-state vocabulary changes dynamically

as a function of both the identity of the target 
computer program and the active-state image data

signals which identify an active state of the target
computer program; and
a speech recognizer (32) for measuring a value of at least
one feature of an utterance during each of a first

sequence of successive time intervals within the first
time period to produce a first series of feature

signals, said speech recognizer comparing the first
series of feature signals to each of the acoustic

command models in the first active-state vocabulary to
generate a match score for the utterance and each

acoustic command model, and said speech recognizer
outputting a command signal corresponding to the

command model from the first active-state vocabulary
having the best match score.
An interactive computer system as claimed in Claim 1,
characterized in that:


the first active-state vocabulary comprises
substantially less than all of the acoustic command

models from the system vocabulary; and
the speech recognizer does not compare the measured
feature signals for the first time period to any

acoustic command model which is not in the first
active-state vocabulary.
An interactive computer system as claimed in Claim 2,
characterized in that:


the means for displaying displays at least a second
active-state image different from the first

active-state image for a second active state occurring
during a second time period different from the first

time period; 
the means for identifying an object identifies an
object displayed in the second active-state image, and

generates from an identified object displayed in the
second active-state image a catalog of one or more

second active-state commands identifying a second
active-state function which can be performed in the

second active state of the target computer program;
the means for identifying a first active-state
vocabulary identifies a second active-state vocabulary

of acoustic command models for the second active state,
the second active-state vocabulary comprising the

acoustic command models from the system vocabulary
representing the second active-state commands, the

second active-state vocabulary being at least partly
different from the first active-state vocabulary; and
the speech recognizer measures the value of at least
one feature of an utterance during each of a second

sequence of time intervals within the second time
period to produce a second series of feature signals,

said speech recognizer comparing the second series of
feature signals for the second time period to each of

the acoustic command models in the second active-state
vocabulary to generate the match score for the

utterance and each acoustic command model, and said
speech recognizer outputting the command signal

corresponding to the command model from the second
active-state vocabulary having the best match score.
An interactive computer system as claimed in Claim 3,
characterized in that the target computer program has

only one active state occurring during each time
period.
An interactive computer system as claimed in Claim 4,
characterized in that the target computer program

comprises an operating system program. 
An interactive computer system as claimed in Claim 5,
characterized in that the target computer program

comprises an application program and an operating
system program.
An interactive computer system as claimed in Claim 6,
characterized in that the target computer program

comprises two or more application programs and an
operating system program.
An interactive computer system as claimed in Claim 6,
characterized in that at least some commands for the

active state identify functions which can be performed
on the identified object in the active-state image for

the active state.
An interactive computer system as claimed in Claim 8,
characterized in that the identified object in an

active-state image comprises one or more of a
character, a word, an icon, a button, a scroll bar, a

slider, a list box, a menu, a check box, a container,

or a notebook.
An interactive computer system as claimed in Claim 9,
characterized in that the speech recognizer outputs two

or more command signals corresponding to the command
models from the active-state vocabulary having the best

match scores for a given time period.
An interactive computer system as claimed in Claim 10,
characterized in that the active-state vocabulary of

acoustic command models for each active state further
comprises a set of global acoustic command models

representing global commands identifying functions
which can be performed in each active state of the

target computer program. 
An interactive computer system as claimed in Claim 11,
characterized in that the means for displaying

comprises a display.
An interactive computer system as claimed in Claim 11,
characterized in that the means for displaying displays

both the active-state image for the active state
occurring during a time period, and at least a portion

of one or more images for program states not occurring
during the time period.
A method of computer interaction comprising:

executing, on a processor, a target computer program
having a series of active program states occurring over

a succession of time periods, said target computer
program generating active state image data signals

representing an active state image for an active state
of the target computer program occurring during each

time period, each active state image containing one or
more objects;
displaying at least a first active-state image for a
first active state occurring during a first time

period;
identifying an object displayed in the first
active-state image, and generating from an identified

object a list of one or more first active-state
commands identifying a first active-state function

which can be performed in the first active state of the
target computer program;
storing a system vocabulary of acoustic command models,
each acoustic command model representing one or more

series of acoustic feature values representing an 
utterance of one or more words associated with the

acoustic command model;
identifying a first active-state vocabulary of acoustic
command models for the first active state, the first

active-state vocabulary comprising the acoustic command
models from the system vocabulary representing the

first active-state commands, wherein the first
active-state vocabulary changes dynamically as a

function of both the identity of the target computer
program and the active-state image data signals which

identify an active state of the target computer
program;
measuring a value of at least one feature of an
utterance during each of a first sequence of time

intervals within the first time period to produce a
first series of feature signals;
comparing the first series of feature signals to each
of the acoustic command models in the first

active-state vocabulary to generate a match score for
the utterance and each acoustic command model; and
outputting a command signal corresponding to the
command model from the first active-state vocabulary

having the best match score.
A method of computer interaction as claimed in Claim 14,
characterized in that:


the first active-state vocabulary comprises
substantially less than all of the acoustic command

models from the system vocabulary; and
the step of comparing does not compare the measured
feature signals for the first time period to any 

acoustic command model which is not in the first
active-state vocabulary.
A method of computer interaction as claimed in Claim
15, further comprising the steps of:


displaying at least a second active-state image
different from the first active-state image for a

second active state occurring during a second time
period different from the first time period;
identifying an object displayed in the second
active-state image, and generating from an identified

object a catalog of one or more second active-state
commands identifying a second active-state function

which can be performed in the second active state of
the target computer program;
identifying a second active-state vocabulary of
acoustic command models for the second active state,

the second active-state vocabulary comprising the
acoustic command models from the system vocabulary

representing the second active-state commands, the
second active-state vocabulary being at least partly

different from the first active-state vocabulary;
measuring the value of at least one feature of an
utterance during each of a second saquence of time

intervals within the second time period to produce a
second series of feature signals;
comparing the second series of feature signals for the
second time period to each of the acoustic command

models in the second active-state vocabulary to
generate the match score for the utterance and each

acoustic command model; and 
outputting a command signal corresponding to the
command model from the second active-state vocabulary

having the best match score.
A method of computer interaction as claimed in Claim
16, characterized in that the target computer program

has only one active state occurring during each time
period.
A method of computer interaction as claimed in Claim
17, characterized in that the target computer program

comprises an operating system program.
A method of computer interaction as claimed in Claim
18, characterized in that the target computer program

comprises an application program and an operating
system program.
A method of computer interaction as claimed in Claim
19, characterized in that the target computer program

comprises two or more application programs and an
operating system program.
A method of computer interaction as claimed in Claim
19, characterized in that at least some commands for

the active-state identify functions which can be
performed on the identified objects in the active-state

image for the active state.
A method of computer interaction as claimed in Claim
21, characterized in that the identified object in the

active-state image comprises one or more of a
character, a word, an icon, a button, a scroll bar, a

slider, a list box, a menu, a check box, a container,
or a notebook.
A method of computer interaction as claimed in Claim
22, characterized in that the step of outputting a

command signal comprises outputting two or more command 
signals corresponding to the command models from the

active-state vocabulary having the best match scores
for a given time period.
A method of computer interaction as claimed in claim
23, characterized in that the vocabulary of

acoustic command models for each active state further
comprises a set of global acoustic command models

representing global commands identifying functions
which can be performed in each active state of the

target computer program.
A method of computer interaction as claimed in Claim
24, further comprising the step of displaying both the

active-state image for the active state occurring

during a time period, and at least a portion of one or
more images for program states not occurring during the

time period.
</CLAIMS>
</TEXT>
</DOC>
