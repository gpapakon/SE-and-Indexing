<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE patent-document PUBLIC "-//MXW//DTD patent-document XML//EN" "http://www.ir-facility.org/dtds/patents/v1.4/patent-document.dtd">
<patent-document ucid="EP-1229485-A2" country="EP" doc-number="1229485" kind="A2" lang="EN" family-id="25097551" status="new" date-produced="20090516" date="20020807">
  <bibliographic-data>
    <publication-reference ucid="EP-1229485-A2" status="new" fvid="23516199">
      <document-id status="new" format="original">
        <country>EP</country>
        <doc-number>1229485</doc-number>
        <kind>A2</kind>
        <date>20020807</date>
      </document-id>
    </publication-reference>
    <application-reference ucid="EP-02250136-A" status="new" is-representative="NO">
      <document-id status="new" format="epo">
        <country>EP</country>
        <doc-number>02250136</doc-number>
        <kind>A</kind>
        <date>20020109</date>
      </document-id>
    </application-reference>
    <priority-claims status="new">
      <priority-claim ucid="US-77321301-A" status="new">
        <document-id status="new" format="epo">
          <country>US</country>
          <doc-number>77321301</doc-number>
          <kind>A</kind>
          <date>20010131</date>
        </document-id>
      </priority-claim>
    </priority-claims>
    <technical-data status="new">
      <classifications-ipcr>
        <classification-ipcr status="new">G06F  17/21        20060101AFI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06F  17/21        20060101CFI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06F  17/30        20060101ALI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06F  17/30        20060101CLI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06K   9/20        20060101A I20051008RMEP        </classification-ipcr>
        <classification-ipcr status="new">G06K   9/20        20060101C I20051008RMEP        </classification-ipcr>
        <classification-ipcr status="new">G06K   9/62        20060101ALI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06K   9/62        20060101CLI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06T   7/00        20060101ALI20051220RMJP        </classification-ipcr>
        <classification-ipcr status="new">G06T   7/00        20060101CLI20051220RMJP        </classification-ipcr>
      </classifications-ipcr>
      <classification-ecla status="new">
        <classification-symbol scheme="EC">G06K   9/20L2</classification-symbol>
      </classification-ecla>
      <invention-title load-source="ep" status="new" lang="DE">Detektion und Gebrauch zusätzlicher Information von einem abgestasteten Dokumentbild</invention-title>
      <invention-title load-source="ep" status="new" lang="EN">Detecting and utilizing add-on information from a scanned document image</invention-title>
      <invention-title load-source="ep" status="new" lang="FR">Détection et utilisation d' information complémentaire appartenant à une image de document balayée</invention-title>
    </technical-data>
    <parties>
      <applicants>
        <applicant status="new" format="epo">
          <addressbook>
            <name>MATSUSHITA ELECTRIC IND CO LTD</name>
            <address>
              <country>JP</country>
            </address>
          </addressbook>
        </applicant>
        <applicant status="new" format="intermediate">
          <addressbook>
            <name>MATSUSHITA ELECTRIC INDUSTRIAL CO., LTD.</name>
          </addressbook>
        </applicant>
        <applicant status="new" format="original">
          <addressbook>
            <last-name>MATSUSHITA ELECTRIC INDUSTRIAL CO., LTD.</last-name>
            <address>
              <street>1006 Oaza-Kadoma</street>
              <city>Kadoma City, Osaka Pref., 571-8501</city>
              <country>JP</country>
            </address>
          </addressbook>
        </applicant>
      </applicants>
      <inventors>
        <inventor status="new" format="epo">
          <addressbook>
            <name>GUO JINHONG KATHERINE</name>
            <address>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor status="new" format="epo">
          <addressbook>
            <name>MA YUE</name>
            <address>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor status="new" format="intermediate">
          <addressbook>
            <name>GUO, JINHONG KATHERINE</name>
          </addressbook>
        </inventor>
        <inventor status="new" format="intermediate">
          <addressbook>
            <name>MA, YUE</name>
          </addressbook>
        </inventor>
        <inventor status="new" format="original">
          <addressbook>
            <last-name>GUO, JINHONG KATHERINE</last-name>
            <address>
              <street>6 Tiffany Court</street>
              <city>West Windsor, New Jersey 08550</city>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor status="new" format="original">
          <addressbook>
            <last-name>MA, YUE</last-name>
            <address>
              <street>6 Tiffany Court</street>
              <city>West Windsor, New Jersey 08550</city>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
      </inventors>
      <agents>
        <agent status="new" format="original">
          <addressbook>
            <last-name>Burrington, Alan Graham Headford</last-name>
            <address>
              <street>Atkinson Burrington 25-29 President Buildings President Way</street>
              <city>Sheffield S4 7UR</city>
              <country>GB</country>
            </address>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <international-convention-data>
      <designated-states>
        <ep-contracting-states>
          <country>DE</country>
          <country>FR</country>
          <country>GB</country>
        </ep-contracting-states>
      </designated-states>
    </international-convention-data>
  </bibliographic-data>
  <abstract load-source="ep" status="new" lang="EN">
    <p>A scanned document image, including add-on information such as
handwritten annotations in addition to printed text lines, is processed by a
handwriting detection method. First, at least one projection histogram is generated
from the scanned document image. A regular pattern that correlates to the printed
text lines is determined from the projection histogram. Second, connected
component analysis is applied to the scanned document image to generate at least
one merged text line. Each merged text line relates to at least one of the
handwritten annotation and the printed text line. By comparing the merged text lines
to the regular pattern of the projection histograms, the printed text lines are
discriminated from the handwritten annotations.
<img id="img-00000001" orientation="unknown" wi="75" img-format="tif" img-content="ad" file="00000001.tif" inline="no" he="125"/></p>
  </abstract>
  <description load-source="ep" status="new" lang="EN">
    <heading>
      <u style="single">BACKGROUND OF THE INVENTION</u>
    </heading>
    <heading>
      <u style="single">1. Field of the Invention</u>
    </heading>
    <p num="0001">This invention relates generally to information processing. More particularly,
the invention relates to methods for discriminating add-on information from a
scanned document image having the add-on information and original text.</p>
    <heading>
      <u style="single">2. Discussion</u>
    </heading>
    <p num="0002">With the fast growth of computer-based systems in the past few decades,
office workers now commonly use computerized word processing and office
business systems to produce, edit and revise documents of all varieties, including
printed text documents, spreadsheets, business presentations and the like. While
these computerized systems include powerful document editing tools, there are
times when it is more expedient to edit or annotate printed documents by simply
writing revisions above the text or in the margins of a printed copy of the document.
Sometimes, for example, the person making edits or annotations may not have
access to an electronic copy of the document and may therefore be unable to use
computerized document editing tools.</p>
    <p num="0003">Moreover, there are also times when the person editing or annotating a
printed document may make handwritten changes or additions to the document and
then later need to have a copy of the document in its original, unedited and
annotated form. Unless an extra copy of the document was previously saved, in its
original form. Anyone who has ever tried to reconstitute a heavily edited document
may manually erasing or covering up the edits and annotations understands how 
tedious and time consuming the process is. Automated methods of separating
handwritten annotations from printed text, if developed, could potentially relieve
much of the tedium.</p>
    <p num="0004">The document reconstitution issue aside, hand-drawn annotations present
other processing challenges, namely, how to identify and use the hand-drawn
annotation to code a document for storage and retrieval in a computerized database.
It would be quite useful, for example, if scanned images of paper documents could
be categorized, stored and retrieved based on handwritten designations placed on
the document prior to scanning. That would allow the user to quickly code a
document by hand, leaving the imaging system with the task of identifying and
reading the coded instructions and storing the document appropriately.</p>
    <p num="0005">In general, detecting and using add-on information from a scanned document
image can be very important because once add-on contents of the document are
obtained, they may exhibit richer information than a static scanned document image.
First, the printed text and possibly graphics reflect the originality of the document
while the add-on contents such as handwritten annotations, stamps etc. reflect the
alteration that has been done to the original document. Second, being able to
differentiate the post-alternation done to the document can be beneficial to a
document management system in several ways. For examples, the separation of
the post-alteration may restore contents of add-on information via OCR/ICR or other
pattern recognition/matching techniques. The history of a document may be
recorded by restoring the original content from a document containing the post-alteration.
Additionally, secure transmissions of original document content without
leaking add-on information, and efficient compression and storage scheme may also
be achieved. In the case where the original document is already stored in the 
database, the copy with add-on information need not be stored entirely in the
database, whereas only add-on information needs to be stored.</p>
    <p num="0006">Several attempts have been made address the need to separate handwritten
annotations from printed text. One of them is a method for compressing images of
bank checks that separates the handwritten annotations from the static check form.
Such a method entirely depends on a document identifier such as a magnetic ink
character recognition (MICR) line in order to separate the handwritten text from the
static check form. However, the requirement of the document identifier limits such
attempts to very specialized fields such as x-rays and NMR images, thereby
increasing the cost and reducing the availability.</p>
    <p num="0007">Other limited applications appear in the field of form processing. For
example, in the form processing, handwritten entries on a form can be extracted
using the standard template. This instant method is useful in processing large
amounts of the forms having the same format such as magazine subscription forms,
account forms, etc. However, the template has to be replaced when different types
of documents are to be processed because the template can only handle a limited
number of the different types of the documents. In reality, a document management
system needs to handle various types of documents such as business letters or
forms, images, fax documents, etc. Thus, the form processing method has limited
use, and may be very time consuming and ineffective.</p>
    <p num="0008">While the above described information processing methods have proven to be
effective for their intended use, it is required that a new automatic separation
technique that truly benefits from the add-on information separation be developed.
Additionally, it would be highly desirable if the new method is not limited to specific 
field/formats, yet provides highly efficient separation of the add-on information from
the original text.</p>
    <heading>
      <u style="single">SUMMARY OF THE INVENTION</u>
    </heading>
    <p num="0009">The present invention addresses the aforementioned need by providing a
method of detecting handwritten annotations from a scanned document image
having a handwritten annotation and at least one printed text line.</p>
    <p num="0010">First, at least one projection histogram is generated from the scanned
document image. A regular pattern that correlates to the printed text lines is
determined from the projection histogram. Second, connected component analysis
is applied to the scanned document image to generate at least one merged text line.
The connected component analysis generates connected components by connecting
dark pixels that are in association with the others on the scanned document image.
Bounding boxes are then generated to encapsulate all of the associated connected
components. Line merging is performed over the scanned image to merge the
bounding boxes that are within a same text line to generate at least one line merged
text line. Each merged text line correlates to at least one of the handwritten
annotations and the printed text lines. By comparing the merged text lines to the
regular pattern of the projection histograms, the printed text lines are discriminated
from the handwritten annotations.</p>
    <p num="0011">For a more complete understanding of the invention, its objects and
advantages, reference may be made to the following specification and to the
accompanying drawings. </p>
    <heading>
      <u style="single">BRIEF DESCRIPTION OF THE DRAWINGS</u>
    </heading>
    <p num="0012">
      <sl>
        <li>Figure 1 is a flow chart depicting the method for detecting and separating
handwritten annotations from a scanned document image according to the present
invention:</li>
        <li>Figure 2 is a sample of a scanned document image having printed text lines
and handwritten annotations, with vertical and horizontal projection histograms
placed onto the scanned document image;</li>
        <li>Figures 3A-B show an example of how connected component analysis is
performed (Figure 3B) over the scanned document image (Figure 3A);</li>
        <li>Figures 4A-B show an example of how line merge is performed (Figure 4B)
on the bounding boxes of the connected components (Figure 4A) in the scanned
document image of Figure 2;</li>
        <li>Figure 5 is a flow chart depicting the steps for legible text line elimination in
accordance with the present invention;</li>
        <li>Figure 6 is a flow chart depicting the steps for further discrimination between
the printed text lines and the handwritten annotations of the scanned document
image in accordance with the present invention;</li>
        <li>Figure 7 illustrates the bounding boxes of the handwritten annotations being
detected and separated from the scanned document image of Figure 2;</li>
        <li>Figure 8 illustrates the extracted handwritten annotations obtained from the
scanned document image of Figure 2;</li>
        <li>Figure 9 illustrates how the invention may be applied to a method for
recording a history of a document having at least one secondary version in
accordance with the present invention; </li>
        <li>Figure 10 illustrates how the invention may be applied to a method for
securing transmission of a document having printed text lines and add-on
information in accordance with the present invention; and</li>
        <li>Figure 11 illustrates how the invention may be applied to an efficient
compression method in accordance with the teachings of the present invention.</li>
      </sl>
    </p>
    <heading>
      <u style="single">DETAILED DESCRIPTION THE PREFERRED EMBODIMENTS</u>
    </heading>
    <p num="0013">Referring to Figure 1 of the drawings, a method <b>10</b> for detecting and
separating add-on handwritten annotations from a scanned document image is
presented in accordance with the teachings of the present invention. The present
invention is applicable to any scanned document image, including images
comprising a handwritten annotation and at least one printed text line. The presently
preferred method is performed as follows. First, at least one projection histogram for
the scanned document image is generated, steps <b>14</b> and <b>16.</b> Typically, a regular
pattern that correlates to the printed text lines maybe derived from the projection
histogram, step <b>18.</b> Second, connected component analysis is performed over the
scanned document image, to generate at least one merged text line, steps <b>20</b> and
<b>22.</b> Each merged text line relates to at least one of the handwritten annotations and
the printed text line. By comparing the merged text lines to the regular pattern of the
projection histograms, the printed text lines are discriminated from the handwritten
annotations, steps <b>24-28.</b></p>
    <p num="0014">More specifically, a document containing the printed text lines and the
handwritten annotations on or near the margin area is scanned, step <b>12.</b> Vertical
and horizontal projection histograms are generated from the scanned document
image, steps <b>14</b> and <b>16.</b> Based on these histograms, the printed text margins are 
determined, step <b>18.</b> At step <b>20,</b> the connected component analysis is performed
over the scanned document image to generate bounding boxes. At step <b>22,</b> line
merge is performed on the bounding boxes of the connected components to
generate at least one merged line. The legible text lines are eliminated at step <b>24</b>
based on the text margins and the merged lines determined at steps <b>20</b> and <b>22,</b>
respectively. After step <b>24,</b> most of the printed text lines are separated from the
handwritten annotations except for a few of the small connected components or the
bounding boxes. A further discrimination between the printed text lines and the
handwritten annotations is performed at step <b>26</b> for detecting the remaining text lines
which have small bounding boxes that are not correlated with the text margins. The
handwritten annotations detected at step <b>26</b> are also separated from the scanned
document image at step <b>28.</b></p>
    <p num="0015">Referring to Figure 2 of the drawings, the projection histograms generated at
steps <b>14</b> and <b>16</b> (Fig. 1) are illustrated in accordance with the present invention. The
vertical and horizontal projection histograms <b>56</b> and <b>62,</b> respectively, are generated
from a scanned document image <b>50</b> having the printed text lines <b>52</b> and the
handwritten annotations <b>54.</b> The first sharp climb <b>58</b> and <b>64</b> or the last sharp drop
<b>60</b> and <b>66</b> in the projection histograms <b>56</b> and <b>62</b> may indicate the margins of the
scanned document image <b>50.</b> More specifically, the first sharp climb <b>58,</b> 64
indicates the largest jump in the threshold of the projection histograms <b>56</b> and <b>62</b>
and represents the transition from the margin to the printed text lines <b>52.</b> Likewise,
the last sharp drop <b>60, 66</b> indicates the largest fall in the threshold of the projection
histograms <b>56</b> and <b>62</b> and represents the transition from the printed text lines <b>52</b> to
the margin, or the white space. </p>
    <p num="0016">As is best illustrated in Figure 2, the printed text lines <b>52</b> are usually aligned
horizontally within the margins of English documents. On the contrary, the
handwritten annotations <b>54</b> appear to be anywhere on the available white space of
the scanned document image <b>50,</b> and more frequently, beyond text margins. It
should be understood that the types of documents are not limited to English
documents. Any document having a regular pattern, for example, Chinese and
Japanese documents having printed text lines that are aligned vertically within the
margins of the document and the handwritten annotations on the available white
space may also be used.</p>
    <p num="0017">The vertical and horizontal projection histograms <b>56</b> and <b>62</b> illustrate the
regularities of the text placement within the document image <b>50.</b> From the horizontal
projection histogram <b>62,</b> left and right margins, or white spaces, are determined by
detecting the first sharp climb <b>64</b> and the last sharp drop <b>66.</b> From the vertical
projection histogram <b>56,</b> the top and bottom margins are determined by the first
sharp climb <b>58</b> and the last sharp drop <b>60.</b> Additionally, the vertical projection
histogram <b>56</b> illustrates regularly patterned intensity levels for each of the printed text
lines. Each of the printed text lines contain two peak intensity levels 57 which
correspond to the top and bottom positions of each of the printed text lines.
Although the handwritten annotations <b>54,</b> written on the margin of the document may
cause ripples in the projection histograms <b>56</b> and <b>62,</b> the margins can still be
detected without effect.</p>
    <p num="0018">Referring to Figure 3A-B of the drawings, connected component analysis is
performed on the scanned document image <b>50.</b> Figure 3A shows an upper portion
of the scanned document image <b>50</b> containing the printed text lines <b>52</b> and the
handwritten annotation <b>54.</b> In the present invention, a fast run-length based 
algorithm is preferred in generating connected components. The run-length based
algorithm detects for a consecutive row of dark pixels and their connectivity with its
neighboring pixels in order to find pixels in association. A more detailed description
of the faster run-length based algorithm is disclosed in U.S. Patent Serial No.
<u style="single">          </u>, filed <u style="single">          </u>, entitled "Run-length Based Connected Components and
Contour Following for Enhancing the performance of Circled Region Extraction
Algorithm", which is assigned to the same assignee as the present invention.
However, it should be understood that another method for performing connected
component analysis may also be used.</p>
    <p num="0019">As is best illustrated in Figure 3B, the bounding boxes of both the printed text
line <b>72</b> and the handwritten annotations <b>74</b> are generated upon completing the
connected component analysis onto the scanned document image <b>50.</b> Each of the
bounding boxes <b>72</b> and <b>74</b> are constructed for merging the connection components
that are in association. The appropriate size of each box is selected so that all dark
pixels that are in consideration reside within the bounding box. After the connected
component analysis, a line merge step <b>22</b> (Fig. 1) is performed on the bounding
boxes <b>72</b> and <b>74</b> of the connected components to merge together individual text
characters within the same line as shown in Figures 4A-B.</p>
    <p num="0020">Figure 4A illustrates the upper portion of the scanned document image <b>50</b>
over which the connected components analysis has been performed. Each of the
printed text lines and the handwritten annotations is encapsulated by the bounding
boxes <b>72</b> and <b>74.</b> The bounding boxes of the printed text lines <b>72</b> correspond to a
more regular pattern. They have similar height and similar top positions for example.
On the contrary, the bounding boxes of the handwritten annotations <b>74</b> are less 
regularly patterned. The handwritten annotation bounding boxes <b>74</b> vary in sizes
and height from the others.</p>
    <p num="0021">The line merge step <b>22</b> arbitrarily chooses two bounding boxes 73, 75 be line
merged, for example, any two from the printed text line bounding boxes 72, two from
the handwritten bounding boxes, 74, or one from the printed text line bounding box
72 and the other one from the handwritten bounding box 74 may arbitrarily be
selected. The algorithm used in the line merge step <b>22</b> considers the top positions of
any two arbitrary bounding boxes, their box height, and their horizontal distance from
each other. If one of the two selected bounding boxes <b>73, 75</b> is completely inside
the other bounding box, the two selected bounding boxes <b>73, 75</b> will be merged.
Also, if two selected bounding boxes <b>73, 75</b> are neighbors and are on the same line,
the two selected bounding boxes will be merged. In other words, the two bounding
boxes will be merged if the horizontal distance between the two selected bounding
boxes is below a pre-determined threshold, measured from the top positions, and if
the heights of the two selected bounding boxes <b>73, 75</b> are similar. The line merge
process is performed recursively until no more line merging can be done. This is
done because two merged bounding boxes may be qualified to merge with a third
bounding box <b>72, 74.</b></p>
    <p num="0022">As illustrated in Figure 4B of the drawings, after line merge, the printed text
lines <b>52</b> (Fig. 1) form long bounding boxes <b>82</b> with uniform height. The regularly
patterned merged bounding boxes <b>82</b> of the printed text lines correspond with twin
peaks <b>57</b> in the vertical projection histogram <b>56</b> (Fig.1). However, as shown in
Figures 4A-B, the bounding boxes of the handwritten annotations <b>84</b> generally
create connected components that vary greatly in size. Therefore, the bounding 
boxes of the handwritten annotations <b>84</b> are typically unlikely to merge and generally
do not correspond with the projection histogram <b>56.</b></p>
    <p num="0023">With reference to Figures 4A-B and 5 of the drawings, the step <b>24</b> of
eliminating the printed text lines is illustrated in greater detail. After the vertical and
horizontal projection histograms <b>56</b> and <b>62</b> have been generated, the peaks that
correspond to each of the merged text line <b>82</b> can be characterized as having the
twin peaks <b>57.</b> The twin peaks <b>57</b> are similar in height and their distance is within a
predetermined range. Each of these twin peaks <b>57</b> represents the top and bottom
position of each of the printed text lines. The distance between the tips of these two
peaks can be used to estimate the height of the printed text line to which it
corresponds. The text line peaks from the vertical projection histogram <b>56</b> are
detected at step <b>90.</b></p>
    <p num="0024">After the text line peaks are detected at step <b>90,</b> the merged lines <b>82</b> and <b>84</b>
that fall in the center position of the peaks are scanned, step <b>92.</b> Each of the
merged lines <b>82</b> and <b>84</b> is examined individually to determine whether all of the
bounding boxes <b>72, 74</b> fall within the margins, step <b>94.</b> If the bounding boxes <b>72, 74</b>
do not reside within the margins, the merged lines <b>82, 84</b> are not eliminated, step <b>96,</b>
and the next merged line <b>82, 84</b> that falls in the center position of the peak is
scanned, step <b>92.</b></p>
    <p num="0025">If the bounding boxes <b>72, 74</b> satisfy the condition of step <b>94,</b> they are further
examined to determine if their heights are near the peak height, step <b>98.</b> If the
bounding boxes <b>72, 74</b> do not have similar height, the scanned merged text lines <b>82</b>
and <b>84</b> are not removed, step <b>100,</b> and the next merged text line that falls in the
center position of peak <b>57</b> is scanned at step <b>92.</b> If the bounding boxes satisfy the
condition of step <b>98,</b> the scanned merged text line is considered as being a printed 
text line and is removed, step <b>102.</b> The elimination process <b>24</b> then goes back to
step <b>92</b> and continues to scan the next merged line boxes until all merged text lines
<b>82</b> and <b>84</b> are scanned, step <b>104.</b></p>
    <p num="0026">When all merged line boxes are examined at step <b>104</b>, the next peak is
detected at step <b>106.</b> The same elimination process is performed on the scanned
document image until all peaks are detected and examined at step <b>108.</b> The
elimination process <b>24</b> makes certain that only possible main text lines <b>52</b> (Fig. 2)
are eliminated. To assure that only main text lines are eliminated, the projection
histogram peak position and the width are cross-checked with the position and
height of the printed text line that is to be eliminated. Only merged text lines <b>82</b> and
<b>84</b> that fall within the margins are considered for removal.</p>
    <p num="0027">Referring to Figure 6, the step <b>26</b> for further discrimination between the
printed text lines <b>52</b> (Fig. 1) and the handwritten annotations <b>54</b> (Fig. 1) is illustrated.
After the text line elimination step <b>24,</b> most of the printed text lines have been
removed, except for a few types of bounding boxes of the printed text lines. For
example, the bonding boxes of some small connected components within the text
line may not have been removed. These small connected components include
punctuation or special symbols within the sentence of abnormal bounding box size.
The bounding boxes of the printed text lines that are very short have lower
probabilities of being detected because their low intensity levels in the projection
histograms do not match the regular pattern of the other printed text lines. On the
contrary, some small connected components within a block of handwritten
annotations may have been removed because they coincidentally fall within the
margins of the printed text lines and align with a detected peak. </p>
    <p num="0028">After the text line elimination step <b>24</b> (Fig. 5), the algorithm determines
whether the remaining components have sizes that are smaller than a predetermined
small size, step <b>110.</b> If the remaining components have sizes that are smaller than
the predetermined small size, then the algorithm determines whether the remaining
components reside either near or inside the removed text lines step <b>112.</b> If the
remaining components reside either near or inside the removed text line, then the
remaining components are considered punctuation or special symbols within the
sentence and are therefore removed, step <b>114.</b></p>
    <p num="0029">If the remaining components do not reside near or inside the removed text
line, the algorithm determines whether the remaining components are isolated from
the other text, step <b>116.</b> If the remaining components are isolated, the remaining
components are considered noise and are removed, step <b>118.</b> If the remaining
components are not isolated, the remaining components are considered as the
handwritten annotations, and left untouched, step <b>120.</b></p>
    <p num="0030">If the remaining components have sizes that are larger than the
predetermined small size at step <b>110,</b> the algorithm determines whether the
remaining components have sizes that are larger than a predetermined large size at
step <b>122.</b> If the remaining components have sizes that are larger than the
predetermined large size, the remaining components are considered as the
handwritten annotations and left untouched, step <b>124.</b></p>
    <p num="0031">If the size of the remaining components are not larger than the predetermined
large size, the height of the bounding boxes of the remaining components are
compared to the height of the majority of the removed text lines and the margins,
step <b>126.</b> If the height of the remaining components are not similar to the height of
the majority of the removed text lines or the bounding boxes of the remaining 
components do not reside within the margin of the document page, the remaining
components are considered to be handwritten annotations and left untouched, step
<b>124.</b></p>
    <p num="0032">If the remaining components satisfy the condition of step <b>126,</b> it is determined
whether the remaining components align with the left margin <b>64</b> (Fig. 2), step <b>128.</b> If
the remaining components do not align with the left margin, the remaining
components are considered to be handwritten annotations and are left untouched,
step <b>124.</b> If the remaining components align with the left margin <b>64,</b> the remaining
components are considered to be short printed text lines having the peak intensity
levels in the vertical projection histogram that are too low to be detected, and are
removed, step <b>130.</b></p>
    <p num="0033">At the further text removal step <b>130,</b> every component deleted from the text
line removal is further examined in order to determine if any connected components
that may belong to the handwritten annotations have been removed. In the present
invention, the bounding boxes that are surrounded by a group of irregular shapes
which are considered handwritten annotations even if these bounding boxes satisfy
all of the above-mentioned conditions. These bounding boxes surrounded by a
group of irregular shapes are then restored at the end of step <b>26</b> (Fig. 1).</p>
    <p num="0034">Referring to Figures 7 and 8 of the drawings, the results of the handwriting
detection and separation from the scanned document image <b>50</b> (Fig. 2) are
illustrated. After the separation step <b>28</b> (Fig. 1), the handwritten annotations are left
as the line merged bounding box <b>85,</b> and printed text lines may either be discarded
or saved into a memory. The handwritten annotations <b>54</b> are then extracted which
may then be either discarded or saved into a memory for future use. </p>
    <p num="0035">Figure 9 illustrates a first embodiment of how the invention may be applied to
a method <b>200</b> for recording a history of a document. The document may have one
and more versions: at least an original version (1<sup>st</sup> version) and maybe revisions
(secondary version). The original version includes only printed text lines whereas
the secondary version includes the printed text lines and secondary handwritten
annotations. When a paper document is scanned, the original version (printed text
lines) may or may not exist in the document database electronically.</p>
    <p num="0036">Starting at step <b>202,</b> a document with mixed handwritten annotation and
printed text lines is scanned. The handwritten annotations of the scanned document
image are discriminated from the printed text lines according to the aforementioned
method for detecting and separating handwritten annotations from the scanned
document image, step <b>204.</b> The extracted printed text lines are compared to all the
original documents in the database at step <b>206</b> in order to determine if an original
version exists in the database, step <b>208.</b> In step <b>206,</b> if there exists an original
document in the database that is identical to the extracted printed text lines, this
document is identified to be the original version of the scanned document image.
Several publications disclose the methods for identifying two same documents
namely duplicate document detection. In "Duplicate document detection" in
Proceedings of Document Recognition IV (IS&amp;T/SPIE Electronic Imaging), pages 88-94,
February, 1997, Spitz employs character shape codes as features and compares
them using a standard string matching algorithm. In "The detection of duplicates in
document image databases" in Proceedings of the Fourth International Conference
on Document Analysis and Recognition, pages 314-318, August 1997, Doermann et
al., use shape codes to address the duplicate document detection problem from full-layout
aspect. </p>
    <p num="0037">If no original document is found to be identical to the extracted printed text
lines, the separated printed text lines will be stored as the original document, step
<b>212.</b> The scanned document image having handwritten annotation and printed text
lines will be recorded as the second version of the original document, step <b>218.</b> The
process is terminated. If the separated printed text lines are related to one original
document at step <b>208,</b> it is further determined whether there is any revisions
(secondary version) of the original document, step <b>210.</b> In case of the identified
original document having no other secondary versions, the scanned image is stored
as a secondary version, step <b>222.</b> The process is terminated. When the identified
original document has at least one revision, the scanned image is stored as the most
recent version, step <b>220.</b> The process is terminated. The above described
application has a particular use, but not limited to, for recording a history of a
document when the document is revised by a group of people at different locations
and times.</p>
    <p num="0038">Figure 10 illustrates a second embodiment <b>300</b> of how the invention may be
applied to a method for securing transmission of an original version of a document.
The document has two versions, namely, the original version and at least one
secondary version. The original version includes printed text lines whereas the
secondary version includes handwritten annotations in addition to the printed text
lines.</p>
    <p num="0039">Starting at step <b>302,</b> the secondary document having the printed text lines and
the handwritten annotations is scanned. The handwritten annotations are then
separated from the printed text lines according to the aforementioned method for
detecting and separating the add-on information 10 (Fig. 1), step <b>304.</b> After the
separation, there are two methods of sending a secured document. The first method, 
step <b>306,</b> sends the extracted printed text lines only to avoid add-on handwritten
annotations being transmitted.</p>
    <p num="0040">Optionally, a second method can be used to transmit the original version of
the document if it exists, step <b>308.</b> In step <b>308,</b> the extracted printed text line is
compared to all the original documents in the database, step <b>310.</b> If the original
document is identified from the database, step <b>312,</b> this identified original document
will be sent instead, step <b>314.</b> If no original document is identified, the extracted
printed text lines will be sent, step <b>306.</b> Sometimes, various versions of the same
original document are to be sent at different times, and slight difference in
handwriting separation results may yield different extracted printed text lines at each
time. When step <b>308</b> is applied, the same original version is sent to the recipient at
every time. The above described application has a particular use, but not limited to,
for securing transmission of documents having handwritten confidential information.</p>
    <p num="0041">Figure 11 illustrates a third embodiment of how the invention is applied to a
compression method, step <b>400.</b> The document includes an original version having
printed text lines that is stored in a database. The document may further include a
secondary version having handwritten annotations in addition to the printed text
lines.</p>
    <p num="0042">Starting at step <b>402,</b> the secondary version of the document having the
printed text lines and the handwritten annotations is scanned. The handwritten
annotations are separated from the printed text lines according to the
aforementioned method for detecting and separating the add-on information 10 (Fig.
1), step <b>404.</b> The separated printed text lines are then compared to all the original
documents in the database at step <b>406</b> in order to find the identical document, step
<b>408.</b> If there does not exist an original document for the printed text lines, the 
extracted printed text line is stored as original document, step <b>410,</b> and handwritten
annotations is stored in a different memory location, step <b>412.</b> The memory location
of the stored handwritten annotation is linked to the memory location of the original
document, step <b>414.</b> The process is terminated. In case of extracted printed text
lines having an original document in the database, only handwritten annotations are
stored, step <b>416,</b> and therefore linked to the identified original document, step <b>418.</b>
The process is terminated. In this case, the extracted printed text lines are ignored,
and memory space is saved.</p>
    <p num="0043">Those skilled in the art can now appreciate from the foregoing description that
the broad teachings of the present invention can be implemented in a variety of
forms. Therefore, while this invention has been described in connection with
particular examples thereof, the true scope of the invention should not be so limited
since other modifications will become apparent to the skilled practitioner upon a
study of the drawings, specification, and following claims.</p>
  </description>
  <claims load-source="ep" status="new" lang="EN">
    <claim num="1">
      <claim-text>A method for detecting handwritten annotations from a scanned
document image, the scanned document image having a handwritten annotation and
at least one printed text line, the method comprising the steps of:
<claim-text><claim-text>generating at least one projection histogram for the scanned document
image, wherein the projection histogram includes a regular pattern that correlates to
the printed text lines;</claim-text><claim-text>applying connected component analysis to the scanned document
image in order to generate at least one merged text line, wherein each of the merged
text lines correlate to at least one of the handwritten annotations and the printed text
line; and</claim-text><claim-text>comparing the merged text lines to the regular pattern of the projection
histograms, thereby discriminating the printed text lines from the handwritten
annotations.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="2">
      <claim-text>The method of claim 1, wherein the regular pattern is margins of the
printed text line.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>The method of claim 1, wherein the projection histogram is selected
from the group consisting of a vertical projection histogram, a horizontal projection
histogram and the combinations thereof.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>The method of claim 1 further comprising the step of detecting text line
peaks from the projection histogram of the scanned document image.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>The method of claim 1, wherein the step of applying connected
component analysis further comprising steps of:
<claim-text><claim-text>generating connected components onto the scanned document image
by connecting dark pixels that are in relation to each other;</claim-text><claim-text>generating bounding boxes of the connected components; and</claim-text><claim-text>line merging the bounding boxes that are within a same text line in
order to generate the merged text line.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="6">
      <claim-text>The method of claim 5, wherein the step of line merging is recursively
performed until there is no bounding box on the same text line to line merge.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>The method of claim 5, wherein the step of line merging merges two of
the bounding boxes if one bounding box is completely inside the other bounding box.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>The method of claim 5, wherein the step of line merging merges two of
the bounding boxes if two bounding boxes are neighbors and on a same line.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>The method of claim 1 further comprising the step of separating the
printed text lines and the handwritten annotations by comparing the size of the
bounding box to the size of the printed text lines.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>A method for detecting handwritten annotations from a scanned
document image, the scanned document image having a handwritten annotation and
at least one printed text line, the method comprising the steps of:
<claim-text><claim-text>generating vertical and horizontal projection histograms for the
scanned document image, wherein the projection histogram includes margins that
correlate to the printed text lines;</claim-text><claim-text>generating connected components by connecting dark pixels that are in
association with the others on the scanned document image;</claim-text><claim-text>generating bounding boxes that encapsulates all of the connected
components that are in relation with each other;</claim-text><claim-text>line merging the bounding boxes that are within a same text line in
order to generate at least one merged text line, wherein each of the merged text
lines correlates to at least one of the handwritten annotations and the printed text
lines;</claim-text><claim-text>detecting text line peaks from the horizontal projection histogram of the
scanned document image; and</claim-text><claim-text>comparing the merged text lines to the margins determined from the
vertical and horizontal projection histograms, thereby discriminating the printed text
lines from the handwritten annotation.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="11">
      <claim-text>A method for recording a history of a document comprising the steps
of:
<claim-text><claim-text>scanning a document having printed text lines and handwritten annotations;</claim-text><claim-text>separating the handwritten annotations from the printed text lines of the
scanned document;</claim-text><claim-text>comparing the scanned document with an original document wherein the
original document includes only the printed text lines;</claim-text><claim-text>determining an existence of previous versions of the document; and</claim-text><claim-text>recording a history of the document based on the separated handwritten
annotations.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="12">
      <claim-text>The method of claim 11 wherein the step of separating the handwritten
annotations from the printed text lines of the scanned document further comprising
the steps of:
<claim-text><claim-text>generating at least one projection histogram for the scanned document
image, wherein the projection histogram includes a regular pattern that correlates to
the printed text lines;</claim-text><claim-text>applying connected component analysis to the scanned document
image in order to generate at least one merged text line, wherein each merged text
line relates to at least one of the handwritten annotations and the printed text line;
and</claim-text><claim-text>comparing the merged text lines to the regular pattern of the projection
histograms, thereby discriminating the printed text lines from the handwritten
annotations.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="13">
      <claim-text>The method of claim 12, wherein the step of applying connected
component analysis further comprising steps of:
<claim-text><claim-text>generating connected components by connecting dark pixels that are in
relation to each other on the scanned document image;</claim-text><claim-text>generating bounding boxes of the connected components; and</claim-text><claim-text>line merging the bounding boxes that are within a same text line in
order to generated the merged text line.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="14">
      <claim-text>The method of claim 11 further comprising the steps of:
<claim-text><claim-text>detecting the existence of the original document;</claim-text><claim-text>storing the printed text lines as the original document when the printed
text lines of the scanned document are different from the detected original document;
and</claim-text><claim-text>recording the scanned document as a second version of the original
document.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="15">
      <claim-text>The method of claim 11, wherein the step of recording the history of the
document further comprising the steps of:
<claim-text><claim-text>recording the scanned document as a second version of the original
document if no previous version of the original document is detected; and</claim-text><claim-text>recording the scanned document as a latest version of the original
document if at least one previous version is detected.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="16">
      <claim-text>A method for securing transmission of an original version of a
document, the document having the original version and at least one secondary
version, wherein the original version includes only printed text lines and the
secondary version includes the printed text lines and handwritten annotations, the
method comprising the steps of:
<claim-text><claim-text>separating the printed text lines from the handwritten annotations in at
least one secondary version of the document;</claim-text><claim-text>determining the original version of the document in relation with the
printed text lines separated from the handwritten annotations; and</claim-text><claim-text>transmitting the printed text lines after the step of determining the
original version of the document.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="17">
      <claim-text>The method of claim 16, wherein the step of separating the printed text
lines from the handwritten annotations further comprising the steps of:
<claim-text><claim-text>generating vertical and horizontal projection histograms for the
scanned document image, wherein the projection histogram includes margins that
correlate to the printed text lines;</claim-text><claim-text>generating connected components by connecting dark pixels that are in
association with the others on the scanned document image;</claim-text><claim-text>generating bounding boxes that encapsulates all of the connected
components that are in relation with each other;</claim-text><claim-text>line merging the bounding boxes that are within a same text line in
order to generate at least one merged text line, wherein each of the merged text
lines correlates to at least one of the handwritten annotations and the printed text
lines;</claim-text><claim-text>detecting text line peaks from the horizontal projection histogram of the
scanned document image; and</claim-text><claim-text>comparing the merged text lines to the margins determined from the
vertical and horizontal projection histograms, thereby discriminating the printed text
lines from the handwritten annotations.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="18">
      <claim-text>The method of claim 17, wherein the step of transmitting the document
further comprising the steps of:
<claim-text><claim-text>transmitting the original version when the printed text lines are identical
to the original version of the document; and</claim-text><claim-text>transmitting the printed text lines when the printed text lines are
different from the original version of the document.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="19">
      <claim-text>An efficient compression method wherein an original version of a
document is stored in a database, the document having at least one secondary
version which includes printed text lines and handwritten annotations, the method
comprising the steps of:
<claim-text><claim-text>separating the printed text lines from the handwritten annotations in at
least one secondary version of the document;</claim-text><claim-text>comparing the printed text lines with the original documents; and</claim-text><claim-text>storing only the handwritten annotations in association with the original
document.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="20">
      <claim-text>The method of claim 19, wherein the step of separating the printed text
lines from the handwritten annotations further comprising the steps of:
<claim-text><claim-text>generating vertical and horizontal projection histograms for the
scanned document image, wherein the projection histogram includes margins that
correlate to the printed text lines;</claim-text><claim-text>generating connected components by connecting dark pixels that are in
association with the others on the scanned document image;</claim-text><claim-text>generating bounding boxes that encapsulates all of the connected
components that are in relation with each other;</claim-text><claim-text>line merging the bounding boxes that are within a same text line in
order to generate at least one merged text line, wherein each of the merged text
lines correlates to at least one of the handwritten annotations and the printed text
lines;</claim-text><claim-text>detecting text line peaks from the horizontal projection histogram of the
scanned document image; and</claim-text><claim-text>comparing the merged text lines to the margins determined from the
vertical and horizontal projection histograms, thereby discriminating the printed text
lines from the handwritten annotations.</claim-text></claim-text></claim-text>
    </claim>
    <claim num="21">
      <claim-text>The method of claim 19, wherein the step of storing the handwritten
annotations further includes a step of linking the handwritten annotation to the
original version in the database.</claim-text>
    </claim>
  </claims>
  <copyright>User acknowledges that the Information Retrieval Facility (IRF) and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws. User acquires no ownership rights to this xml including but not limited to its format. User hereby accepts the terms and conditions of the Licence Agreement set forth at http://www.ir-facility.org/legal/marec/data_licence</copyright>
</patent-document>
